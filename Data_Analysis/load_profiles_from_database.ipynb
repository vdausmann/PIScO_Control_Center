{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pandas.errors import EmptyDataError\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import text\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup Database\n",
    "#Setup connection parameters\n",
    "username = 'plankton'\n",
    "password = 'piscodisco'\n",
    "host = 'localhost'  # or the IP address of your database server\n",
    "port = '5432'       # default port for PostgreSQL\n",
    "database = 'pisco_crops_db'\n",
    "\n",
    "# Create an engine that connects to the PostgreSQL server\n",
    "engine = create_engine(f'postgresql://{username}:{password}@{host}:{port}/{database}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['M181-267-1_CTD-070_00deg00S-035deg00W_20220516-0335', 'M181-200-1_CTD-055_00deg00S-023deg00W_20220511-0023', 'M181-227-1_CTD-060_00deg00S-027deg00W_20220512-1748', 'M181-126-1_CTD-040_00deg00S-010deg00W_20220505-1401', 'M181-107-1_CTD-036_00deg00S-007deg00W_20220504-0756', 'M181-175-1_CTD-050_00deg00S-019deg00W_20220509-0543', 'M181-140-1_CTD-043_00deg00S-013deg00W_20220506-1822', 'M181-160-1_CTD-047_00deg00S-016deg00W_20220508-0038', 'M181-285-1_CTD-075_00deg00S-039deg00W_20220517-2024', 'M181-295-1_CTD-080_00deg00S-041deg00W_20220518-2053', 'M181-245-1_CTD-065_00deg00S-031deg00W_20220514-0907', 'M181-220-1_CTD-059_00deg00S-026deg00W_20220512-0920', 'M181-216-1_CTD-058_00deg00S-025deg00W_20220511-2352']\n",
      "13 entries found\n"
     ]
    }
   ],
   "source": [
    "# Fetch all table names in the database\n",
    "query_tables = \"SELECT tablename FROM pg_tables WHERE schemaname='public'\"\n",
    "with engine.connect() as conn:\n",
    "    table_names = pd.read_sql_query(text(query_tables), conn)['tablename'].tolist()\n",
    "print(table_names)\n",
    "print(len(table_names),'entries found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database Analysis:\n",
      "================================================================================\n",
      "                                         Table Name  Total Rows  Total Columns  Unique obj_ids  Duplicate obj_ids  Missing Values Memory Usage\n",
      "M181-267-1_CTD-070_00deg00S-035deg00W_20220516-0335       78602             64           78602                  0               0     37.33 MB\n",
      "M181-200-1_CTD-055_00deg00S-023deg00W_20220511-0023      117528             64          117511                 17               0     55.82 MB\n",
      "M181-227-1_CTD-060_00deg00S-027deg00W_20220512-1748      112421             64          112349                 72               0     53.39 MB\n",
      "M181-126-1_CTD-040_00deg00S-010deg00W_20220505-1401      117343             64          117337                  6               0     55.73 MB\n",
      "M181-107-1_CTD-036_00deg00S-007deg00W_20220504-0756      196612             64          196602                 10               0     93.38 MB\n",
      "M181-175-1_CTD-050_00deg00S-019deg00W_20220509-0543      105198             64          105195                  3               0     49.96 MB\n",
      "M181-140-1_CTD-043_00deg00S-013deg00W_20220506-1822       71465             64           71463                  2               0     33.94 MB\n",
      "M181-160-1_CTD-047_00deg00S-016deg00W_20220508-0038      104971             64          104967                  4               0     49.85 MB\n",
      "M181-285-1_CTD-075_00deg00S-039deg00W_20220517-2024       46641             64           46623                 18               0     22.15 MB\n",
      "M181-295-1_CTD-080_00deg00S-041deg00W_20220518-2053       92737             64           92734                  3               0     44.04 MB\n",
      "M181-245-1_CTD-065_00deg00S-031deg00W_20220514-0907      130670             64          130670                  0               0     62.06 MB\n",
      "M181-220-1_CTD-059_00deg00S-026deg00W_20220512-0920       98464             66           98461                  3               0     48.27 MB\n",
      "M181-216-1_CTD-058_00deg00S-025deg00W_20220511-2352      510534             66          510473                 61               0    250.26 MB\n"
     ]
    }
   ],
   "source": [
    "def analyze_table(engine, table_name):\n",
    "    \"\"\"Analyze a single table from the database\"\"\"\n",
    "    query = f'SELECT * FROM \"{table_name}\"'\n",
    "    df = pd.read_sql_query(query, engine)\n",
    "    \n",
    "    analysis = {\n",
    "        'Table Name': table_name,\n",
    "        'Total Rows': len(df),\n",
    "        'Total Columns': len(df.columns),\n",
    "        'Unique obj_ids': len(df['obj_id'].unique()),\n",
    "        'Duplicate obj_ids': df.duplicated(subset='obj_id').sum(),\n",
    "        'Missing Values': df.isnull().sum().sum(),\n",
    "        'Memory Usage': f\"{df.memory_usage().sum() / 1024**2:.2f} MB\"\n",
    "    }\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "column_mapping = {\n",
    "    'area_exc': 'object_area_exc',\n",
    "    'area_rprops': 'object_area_rprops',\n",
    "    '%area': 'object_%area',\n",
    "    'major': 'object_major_axis_len',\n",
    "    'minor': 'object_minor_axis_len', \n",
    "    'centroid_y': 'object_centroid_y',\n",
    "    'centroid_x': 'object_centroid_x',\n",
    "    'convex_area': 'object_convex_area',\n",
    "    'min_intensity': 'object_min_intensity',\n",
    "    'max_intensity': 'object_max_intensity',\n",
    "    'mean_intensity': 'object_mean_intensity',\n",
    "    'intden': 'object_int_density',\n",
    "    'perim.': 'object_perimeter',\n",
    "    'elongation': 'object_elongation',\n",
    "    'range': 'object_range',\n",
    "    'perimareaexc': 'object_perim_area_excl',\n",
    "    'perimmajor': 'object_perim_major',\n",
    "    'circex': 'object_circularity_area_excl',\n",
    "    'angle': 'object_angle',\n",
    "    'bounding_box_area': 'object_boundbox_area',\n",
    "    'eccentricity': 'object_eccentricity',\n",
    "    'equivalent_diameter': 'object_equivalent_diameter',\n",
    "    'euler_number': 'object_euler_nr',\n",
    "    'extent': 'object_extent',\n",
    "    'local_centroid_col': 'object_local_centroid_col',\n",
    "    'local_centroid_row': 'object_local_centroid_row',\n",
    "    'solidity': 'object_solidity',\n",
    "    'circ.': 'object_circularity'\n",
    "}\n",
    "# Analyze all tables\n",
    "results = []\n",
    "for table_name in table_names:\n",
    "    results.append(analyze_table(engine, table_name))\n",
    "    # To rename columns in your first DataFrame:\n",
    "    query = f'SELECT * FROM \"{table_name}\"'\n",
    "    df = pd.read_sql_query(query, engine)\n",
    "    df = df.rename(columns=column_mapping)\n",
    "    df.to_sql(table_name, engine, if_exists='replace', index=False)\n",
    "\n",
    "# Create a DataFrame with the results\n",
    "analysis_df = pd.DataFrame(results)\n",
    "print(\"Database Analysis:\")\n",
    "print(\"=\" * 80)\n",
    "print(analysis_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M181-267-1_CTD-070_00deg00S-035deg00W_20220516-0335 78602\n",
      "78602 64\n",
      "0\n",
      "M181-200-1_CTD-055_00deg00S-023deg00W_20220511-0023 117528\n",
      "117528 64\n",
      "17\n",
      "M181-227-1_CTD-060_00deg00S-027deg00W_20220512-1748 112421\n",
      "112421 64\n",
      "72\n",
      "M181-126-1_CTD-040_00deg00S-010deg00W_20220505-1401 117343\n",
      "117343 64\n",
      "6\n",
      "M181-107-1_CTD-036_00deg00S-007deg00W_20220504-0756 196612\n",
      "196612 64\n",
      "10\n",
      "M181-175-1_CTD-050_00deg00S-019deg00W_20220509-0543 105198\n",
      "105198 64\n",
      "3\n",
      "M181-140-1_CTD-043_00deg00S-013deg00W_20220506-1822 71465\n",
      "71465 64\n",
      "2\n",
      "M181-160-1_CTD-047_00deg00S-016deg00W_20220508-0038 104971\n",
      "104971 64\n",
      "4\n",
      "M181-285-1_CTD-075_00deg00S-039deg00W_20220517-2024 46641\n",
      "46641 64\n",
      "18\n",
      "M181-295-1_CTD-080_00deg00S-041deg00W_20220518-2053 92737\n",
      "92737 64\n",
      "3\n",
      "M181-245-1_CTD-065_00deg00S-031deg00W_20220514-0907 130670\n",
      "130670 64\n",
      "0\n",
      "M181-220-1_CTD-059_00deg00S-026deg00W_20220512-0920 98464\n",
      "98464 66\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "for table_name in table_names:\n",
    "    query = f'SELECT * FROM \"{table_name}\"'\n",
    "    df = pd.read_sql_query(query, engine)\n",
    "    print(table_name, len(df['obj_id']))\n",
    "    print(len(df.index), len(df.columns))\n",
    "    #print(df.columns)\n",
    "    # assuming df is your DataFrame and 'col' is the column you want to count duplicates in\n",
    "    duplicates = df.duplicated(subset='obj_id')\n",
    "\n",
    "    # count the duplicates\n",
    "    num_duplicates = duplicates.sum()\n",
    "\n",
    "    print(num_duplicates)\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10409\n",
      "Index(['date-time', 'pressure [dbar]', 'depth [m]', 'temperature', 'index',\n",
      "       'img_id', 'filename', 'mean_raw', 'std_raw', 'mean', 'std', 'area', 'x',\n",
      "       'y', 'w', 'h', 'saved', 'object_bound_box_w', 'object_bound_box_h',\n",
      "       'bound_box_x', 'bound_box_y', 'object_circularity', 'object_area_exc',\n",
      "       'object_area_rprops', 'object_%area', 'object_major_axis_len',\n",
      "       'object_minor_axis_len', 'object_centroid_y', 'object_centroid_x',\n",
      "       'object_convex_area', 'object_min_intensity', 'object_max_intensity',\n",
      "       'object_mean_intensity', 'object_int_density', 'object_perimeter',\n",
      "       'object_elongation', 'object_range', 'object_perim_area_excl',\n",
      "       'object_perim_major', 'object_circularity_area_excl', 'object_angle',\n",
      "       'object_boundbox_area', 'object_eccentricity',\n",
      "       'object_equivalent_diameter', 'object_euler_nr', 'object_extent',\n",
      "       'object_local_centroid_col', 'object_local_centroid_row',\n",
      "       'object_solidity', 'full_path', 'esd', 'overview_path',\n",
      "       'interpolated_s', 'interpolated_o', 'interpolated_t',\n",
      "       'interpolated_chl', 'interpolated_z_factor', 'TAG_event', 'TT', 'T1',\n",
      "       'T2', 'TH', 'relock', 'restart', 'obj_id', 'part_based_filter'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = f'SELECT * FROM \"M181-220-1_CTD-059_00deg00S-026deg00W_20220512-0920\"'\n",
    "df = pd.read_sql_query(query, engine)\n",
    "\n",
    "print(len(df['date-time'].drop_duplicates()))\n",
    "\n",
    "print(df.columns)\n",
    "len(df.columns)\n",
    "\n",
    "#df['overview_path'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns only in first DataFrame: {'width', 'umap_y', 'height', 'by', 'umap_x', 'bx'}\n",
      "Columns only in second DataFrame: {'bound_box_x', 'std_raw', 'object_bound_box_w', 'std', 'bound_box_y', 'object_bound_box_h', 'mean_raw', 'mean'}\n"
     ]
    }
   ],
   "source": [
    "# To check which columns are unique to each DataFrame:\n",
    "#df1_columns = set(df.columns)\n",
    "df2_columns = set(df.columns)\n",
    "\n",
    "print(\"Columns only in first DataFrame:\", df1_columns - df2_columns)\n",
    "print(\"Columns only in second DataFrame:\", df2_columns - df1_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique = df[['date-time', 'pressure [dbar]', 'depth [m]', 'img_id','temperature','overview_path','interpolated_s','interpolated_t','interpolated_o','interpolated_chl','interpolated_z_factor','restart','relock','TAG_event']].drop_duplicates()\n",
    "#df_unique = df[['date-time']].drop_duplicates()\n",
    "#df_count = df.groupby('date-time').size().reset_index(name='count')\n",
    "#df_unique = df_unique.merge(df_count, on='date-time', how='left')\n",
    "#df_unique = df_unique.sort_values('pressure [dbar]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7759"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum value in \"esd\" column: 609.71\n",
      "Corresponding value in \"area\" column: 400.5\n"
     ]
    }
   ],
   "source": [
    "# Find the index of the minimum value in 'esd' column\n",
    "index = df['esd'].idxmin()\n",
    "\n",
    "# Use .iloc[] to get the corresponding value in the 'area' column for the minimum index\n",
    "min_value_area = df.iloc[index]['area']\n",
    "\n",
    "print('Minimum value in \"esd\" column:', df.iloc[index]['esd'])\n",
    "print('Corresponding value in \"area\" column:', min_value_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           date-time  pressure [dbar]  depth [m]  temperature  index  img_id  \\\n",
      "0  20220505-14191884            -0.32     -0.318        28.51     17       1   \n",
      "1  20220505-14191884            -0.32     -0.318        28.51     73       1   \n",
      "2  20220505-14191884            -0.32     -0.318        28.51     93       1   \n",
      "3  20220505-14191884            -0.32     -0.318        28.51    102       1   \n",
      "4  20220505-14191884            -0.32     -0.318        28.51    106       1   \n",
      "\n",
      "                                      filename     area       x       y  ...  \\\n",
      "0   20220505-14191884_000.968bar_28.51C_17.png   1556.0   432.5  2514.5  ...   \n",
      "1   20220505-14191884_000.968bar_28.51C_73.png    547.5  2166.0  2325.0  ...   \n",
      "2   20220505-14191884_000.968bar_28.51C_93.png    738.5  2095.0  2286.5  ...   \n",
      "3  20220505-14191884_000.968bar_28.51C_102.png    501.5   663.0  2276.0  ...   \n",
      "4  20220505-14191884_000.968bar_28.51C_106.png  14396.5   414.0  2325.0  ...   \n",
      "\n",
      "        angle  bounding_box_area  eccentricity equivalent_diameter  \\\n",
      "0  146.316481             4345.0      0.959131           45.765479   \n",
      "1  161.838256             2016.0      0.954410           28.232038   \n",
      "2  170.915404             2162.0      0.989125           32.800723   \n",
      "3  177.827410              884.0      0.623904           26.558891   \n",
      "4   38.155800            20448.0      0.679303          136.347243   \n",
      "\n",
      "   euler_number    extent  local_centroid_col  local_centroid_row  solidity  \\\n",
      "0           1.0  0.378596           44.913070           22.085714  0.961426   \n",
      "1           1.0  0.310516           25.388179           18.939297  0.554473   \n",
      "2           1.0  0.390842           44.953846            8.415385  0.708893   \n",
      "3           1.0  0.626697           16.846570           12.456679  0.860248   \n",
      "4           1.0  0.714055           73.879529           69.681118  0.971134   \n",
      "\n",
      "                                       overview_path  \n",
      "0  /media/plankton/30781fe1-cea5-4503-ae00-1986be...  \n",
      "1  /media/plankton/30781fe1-cea5-4503-ae00-1986be...  \n",
      "2  /media/plankton/30781fe1-cea5-4503-ae00-1986be...  \n",
      "3  /media/plankton/30781fe1-cea5-4503-ae00-1986be...  \n",
      "4  /media/plankton/30781fe1-cea5-4503-ae00-1986be...  \n",
      "\n",
      "[5 rows x 53 columns]\n"
     ]
    }
   ],
   "source": [
    "def modify_full_path(path):\n",
    "    dirname, base_name = os.path.split(path)\n",
    "    base_parts = base_name.split('_')\n",
    "    new_base_name = '_'.join(base_parts[:-1]) + '.png'\n",
    "    return os.path.join(dirname.replace('Crops', 'Images'), new_base_name)\n",
    "\n",
    "df['overview_path'] = df['full_path'].apply(modify_full_path)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64                date-time  pressure [dbar]  depth [m]  img_id  temperature  \\\n",
      "11770  20220518-21073407             3.27      3.250    1102        28.88   \n",
      "11792  20220518-21073432             3.29      3.270    1103        28.88   \n",
      "11810  20220518-21073457             3.29      3.270    1104        28.88   \n",
      "11826  20220518-21073482             3.27      3.250    1105        28.90   \n",
      "80703  20220518-21162510           413.42    410.736    3226         9.47   \n",
      "...                  ...              ...        ...     ...          ...   \n",
      "90801  20220518-22025680          3433.15   3386.777   14391         2.47   \n",
      "90808  20220518-22025705          3433.33   3386.953   14392         2.47   \n",
      "90816  20220518-22025730          3433.56   3387.182   14393         2.47   \n",
      "90835  20220518-22025755          3433.76   3387.380   14394         2.47   \n",
      "90857  20220518-22025780          3434.01   3387.620   14395         2.47   \n",
      "\n",
      "                                           overview_path  interpolated_s  \\\n",
      "11770  /media/plankton/30781fe1-cea5-4503-ae00-1986be...       35.664473   \n",
      "11792  /media/plankton/30781fe1-cea5-4503-ae00-1986be...       35.664471   \n",
      "11810  /media/plankton/30781fe1-cea5-4503-ae00-1986be...       35.664471   \n",
      "11826  /media/plankton/30781fe1-cea5-4503-ae00-1986be...       35.664473   \n",
      "80703  /media/plankton/30781fe1-cea5-4503-ae00-1986be...       34.628202   \n",
      "...                                                  ...             ...   \n",
      "90801  /media/plankton/30781fe1-cea5-4503-ae00-1986be...       34.912130   \n",
      "90808  /media/plankton/30781fe1-cea5-4503-ae00-1986be...       34.912166   \n",
      "90816  /media/plankton/30781fe1-cea5-4503-ae00-1986be...       34.912100   \n",
      "90835  /media/plankton/30781fe1-cea5-4503-ae00-1986be...       34.912100   \n",
      "90857  /media/plankton/30781fe1-cea5-4503-ae00-1986be...       34.912299   \n",
      "\n",
      "       interpolated_t  interpolated_o  interpolated_chl  \\\n",
      "11770       28.769562        198.4515          -0.01410   \n",
      "11792       28.769574        198.4005          -0.01670   \n",
      "11810       28.769574        198.4005          -0.01670   \n",
      "11826       28.769562        198.4515          -0.01410   \n",
      "80703        7.424378        148.4344          -0.13984   \n",
      "...               ...             ...               ...   \n",
      "90801        2.539670        258.3620          -0.13345   \n",
      "90808        2.539634        258.2684          -0.13399   \n",
      "90816        2.539700        258.4400          -0.13300   \n",
      "90835        2.539700        258.4400          -0.13300   \n",
      "90857        2.539502        257.9278          -0.13621   \n",
      "\n",
      "       interpolated_z_factor  restart  relock TAG_event  \n",
      "11770               0.993783    False    True         1  \n",
      "11792               0.993817    False    True         1  \n",
      "11810               0.993817    False    True         1  \n",
      "11826               0.993783    False    True         1  \n",
      "80703               0.993507    False    True         2  \n",
      "...                      ...      ...     ...       ...  \n",
      "90801               0.986493    False    True        10  \n",
      "90808               0.986492    False    True        10  \n",
      "90816               0.986493    False    True        10  \n",
      "90835               0.986493    False    True        10  \n",
      "90857               0.986491    False    True        10  \n",
      "\n",
      "[64 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "restart_rows = df_unique[df_unique['relock'] == True]\n",
    "print(len(restart_rows), restart_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [05:04<00:00, 27.65s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for table_name in tqdm(table_names):\n",
    "    query = f'SELECT * FROM \"{table_name}\"'\n",
    "    df = pd.read_sql_query(query, engine)\n",
    "    df['overview_path'] = df['full_path'].apply(modify_full_path)\n",
    "    df.to_sql(table_name, engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add log information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to reformat timestamp\n",
    "# def reformat_timestamp(timestamp):\n",
    "#     # Format: YYYYMMDD_HHh_MMm_SSs to YYYYMMDD-HHMMSS\n",
    "#     formatted_timestamp = re.sub(r'(\\d{4})(\\d{2})(\\d{2})_(\\d{2})h_(\\d{2})m_(\\d{2})s', r'\\1\\2\\3-\\4\\5\\6', timestamp)\n",
    "#     return formatted_timestamp\n",
    "\n",
    "# def parse_line(line, row):\n",
    "#     if line.startswith(\"b'TT\"):\n",
    "#         temp_values = line[2:].rstrip(\"'\").split('_')\n",
    "#         row['TT']= float(temp_values[1])\n",
    "#         row['T1']= float(temp_values[3])\n",
    "#         row['T2']= float(temp_values[5])\n",
    "#         row['TH']= float(temp_values[7])\n",
    "\n",
    "#     # # elif line.startswith(\"b'Heat\"):\n",
    "#     # #     return {}\n",
    "#     # # elif line.startswith(\"b'Cool\"):\n",
    "#     # #     return {}\n",
    "#     # else:\n",
    "#     #     return {}\n",
    "\n",
    "# def create_log_df(file_path):\n",
    "\n",
    "#     # Read the log file\n",
    "#     with open(file_path, 'r') as f:\n",
    "#         lines = f.readlines()\n",
    "\n",
    "#     data = []\n",
    "#     temp_data = {}\n",
    "\n",
    "#     # Iterate through lines\n",
    "#     for line in lines:\n",
    "#         line = line.rstrip('\\n')\n",
    "#         # Check if line is a timestamp\n",
    "#         if \"h\" in line and \"m\" in line and \"s\" in line:\n",
    "#             if temp_data:\n",
    "#                 # If current timestamp already exists in data, update the existing dictionary\n",
    "#                 if any(d['timestamp'] == temp_data['timestamp'] for d in data):\n",
    "#                     existing_data = [d for d in data if d['timestamp'] == temp_data['timestamp']][0]\n",
    "#                     existing_data.update(temp_data)\n",
    "#                 else:\n",
    "#                     data.append(temp_data)\n",
    "#                 temp_data = {}\n",
    "#             temp_data['timestamp'] = line\n",
    "#         else:\n",
    "#             # Parse line according to message type\n",
    "#             if line.startswith(\"b'TT\"):\n",
    "#                 temp_values = line[2:].rstrip(\"'\").split('_')\n",
    "#                 temp_data['TT'] = float(temp_values[1])\n",
    "#                 temp_data['T1'] = float(temp_values[3])\n",
    "#                 temp_data['T2'] = float(temp_values[5])\n",
    "#                 temp_data['TH'] = float(temp_values[7])\n",
    "#             elif line.startswith('Restart Tag'):\n",
    "#                 temp_data['restart'] = True\n",
    "#             elif line == 'Relock':\n",
    "#                 temp_data['relock'] = True\n",
    "\n",
    "#     # If there is data waiting after the last line, add it\n",
    "#     if temp_data:\n",
    "#         if any(d['timestamp'] == temp_data['timestamp'] for d in data):\n",
    "#             existing_data = [d for d in data if d['timestamp'] == temp_data['timestamp']][0]\n",
    "#             existing_data.update(temp_data)\n",
    "#         else:\n",
    "#             data.append(temp_data)\n",
    "\n",
    "#     # Create a dataframe from the data list\n",
    "#     df = pd.DataFrame(data)\n",
    "\n",
    "#     # Convert timestamps to datetime and set it as index\n",
    "#     df['timestamp'] = pd.to_datetime(df['timestamp'], format=\"%Y%m%d_%Hh_%Mm_%Ss\")\n",
    "#     df.set_index('timestamp', inplace=True)\n",
    "\n",
    "#     # Replace NaN values in 'relock' and 'restart' columns with False\n",
    "#     df[['restart', 'relock']] = df[['restart', 'relock']].fillna(False)\n",
    "#     cols = ['TT', 'T1', 'T2', 'TH']  # list of your column names\n",
    "#     for col in cols:\n",
    "#         df[col] = df[col].interpolate()\n",
    "    \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "## new create log function\n",
    "def create_log_df(file_path):\n",
    "    # Read the log file\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    data = []\n",
    "    temp_data = {}\n",
    "    indicator = 0\n",
    "    counter = 1\n",
    "\n",
    "    # Iterate through lines\n",
    "    for line in lines:\n",
    "        line = line.rstrip('\\n')\n",
    "        #print(line)\n",
    "        # Check if line is a timestamp\n",
    "        if \"h\" in line and \"m\" in line and \"s\" in line:\n",
    "            if temp_data and 'timestamp' in temp_data:\n",
    "                # If current timestamp already exists in data, update the existing dictionary\n",
    "                if any(d['timestamp'] == temp_data['timestamp'] for d in data):\n",
    "                    existing_data = [d for d in data if d['timestamp'] == temp_data['timestamp']][0]\n",
    "                    existing_data.update(temp_data)\n",
    "                else:\n",
    "                    data.append(temp_data)\n",
    "                temp_data = {}\n",
    "            temp_data['timestamp'] = line\n",
    "        else:\n",
    "            # Parse line according to message type\n",
    "            if line.startswith(\"b'TT\"):\n",
    "                temp_values = line[2:].rstrip(\"'\").split('_')\n",
    "                temp_data['TT'] = float(temp_values[1])\n",
    "                temp_data['T1'] = float(temp_values[3])\n",
    "                temp_data['T2'] = float(temp_values[5])\n",
    "                temp_data['TH'] = float(temp_values[7])\n",
    "            elif line.startswith('Restart Tag'):\n",
    "                temp_data['restart'] = True\n",
    "                indicator = 0\n",
    "            elif line == 'Relock':\n",
    "                temp_data['relock'] = True\n",
    "                indicator = counter\n",
    "                counter += 1\n",
    "            temp_data['TAG_event'] = indicator\n",
    "\n",
    "    # If there is data waiting after the last line, add it\n",
    "    if temp_data:\n",
    "        if any(d['timestamp'] == temp_data['timestamp'] for d in data):\n",
    "            existing_data = [d for d in data if d['timestamp'] == temp_data['timestamp']][0]\n",
    "            existing_data.update(temp_data)\n",
    "        else:\n",
    "            data.append(temp_data)\n",
    "\n",
    "    # Create a dataframe from the data list\n",
    "    df = pd.DataFrame(data)\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], format=\"%Y%m%d_%Hh_%Mm_%Ss\")\n",
    "    df.set_index('timestamp', inplace=True)\n",
    "\n",
    "    # Replace NaN values in 'relock' and 'restart' columns with False\n",
    "    df[['restart', 'relock']] = df[['restart', 'relock']].fillna(False)\n",
    "    cols = ['TT', 'T1', 'T2', 'TH']  # list of your column names\n",
    "    for col in cols:\n",
    "        df[col] = df[col].interpolate()\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closest logfile: 20220512_17h_47m__Templog.txt, Size: 509203 bytes\n",
      "Updated M181-227-1_CTD-060_00deg00S-027deg00W_20220512-1748\n",
      "Closest logfile: 20220505_14h_01m__Templog.txt, Size: 821675 bytes\n",
      "Updated M181-126-1_CTD-040_00deg00S-010deg00W_20220505-1401\n",
      "Closest logfile: 20220504_07h_56m__Templog.txt, Size: 846382 bytes\n",
      "Updated M181-107-1_CTD-036_00deg00S-007deg00W_20220504-0756\n",
      "Closest logfile: 20220509_05h_43m__Templog.txt, Size: 717942 bytes\n",
      "Updated M181-175-1_CTD-050_00deg00S-019deg00W_20220509-0543\n",
      "Closest logfile: 20220516_03h_34m__Templog.txt, Size: 761965 bytes\n",
      "Updated M181-267-1_CTD-070_00deg00S-035deg00W_20220516-0335\n",
      "Closest logfile: 20220506_18h_22m__Templog.txt, Size: 772615 bytes\n",
      "Updated M181-140-1_CTD-043_00deg00S-013deg00W_20220506-1822\n",
      "Closest logfile: 20220508_00h_38m__Templog.txt, Size: 568372 bytes\n",
      "Updated M181-160-1_CTD-047_00deg00S-016deg00W_20220508-0038\n",
      "Closest logfile: 20220517_20h_24m__Templog.txt, Size: 627455 bytes\n",
      "Updated M181-285-1_CTD-075_00deg00S-039deg00W_20220517-2024\n",
      "Closest logfile: 20220518_20h_52m__Templog.txt, Size: 601286 bytes\n",
      "Updated M181-295-1_CTD-080_00deg00S-041deg00W_20220518-2053\n",
      "Closest logfile: 20220514_09h_06m__Templog.txt, Size: 735741 bytes\n",
      "Updated M181-245-1_CTD-065_00deg00S-031deg00W_20220514-0907\n",
      "Closest logfile: 20220511_00h_23m__Templog.txt, Size: 668470 bytes\n",
      "Updated M181-200-1_CTD-055_00deg00S-023deg00W_20220511-0023\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "directory = '/media/plankton/30781fe1-cea5-4503-ae00-1986beb935d2/Segmentation_results/M181/Templog/'\n",
    "\n",
    "# find the right logfile for each profile\n",
    "for profile in table_names:\n",
    "    timestamp = profile[-13:]\n",
    "    # Convert timestamp to datetime object\n",
    "    date_time_obj = datetime.datetime.strptime(timestamp, '%Y%m%d-%H%M')\n",
    "    min_diff = datetime.timedelta(days=365*1000)  # initialize with a big time difference\n",
    "    closest_file = None\n",
    "\n",
    "    # Iterate over all files in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        # Check if filename is a Templog\n",
    "        if '__Templog.txt' in filename:\n",
    "            # Extract timestamp from filename and convert to datetime object\n",
    "            file_timestamp = filename[:16]\n",
    "            file_datetime = datetime.datetime.strptime(file_timestamp, '%Y%m%d_%Hh_%Mm')\n",
    "\n",
    "            # Calculate time difference\n",
    "            diff = abs(date_time_obj - file_datetime)\n",
    "\n",
    "            # If this file is closer, update min_diff and closest_file\n",
    "            if diff < min_diff:\n",
    "                min_diff = diff\n",
    "                closest_file = filename\n",
    "\n",
    "    if closest_file is None:\n",
    "        print(\"Logfile not found\")\n",
    "    else:\n",
    "        file_path = os.path.join(directory, closest_file)\n",
    "        file_size = os.path.getsize(file_path)  # Get file size in bytes\n",
    "        print(f\"Closest logfile: {closest_file}, Size: {file_size} bytes\")\n",
    "    \n",
    "    # Read the log file and parse the relevant data\n",
    "\n",
    "    df_log = create_log_df(file_path)\n",
    "\n",
    "    # Match the data with the profile dataframe\n",
    "    query = f'SELECT * FROM \"{profile}\"'\n",
    "    df = pd.read_sql_query(query, engine)\n",
    "    df.drop(['TT_x', 'T1_x', 'T2_x', 'TH_x', 'restart_x', 'relock_x', 'Time_log_x', 'TT_y', 'T1_y', 'T2_y', 'TH_y', 'restart_y', 'relock_y', 'Time_log_y', 'TT', 'T1', 'T2', 'TH', 'restart', 'relock', 'Time_log', 'TAG_event', 'TAG_event_x', 'TAG_event_y', 'near_event', 'near_event_x', 'near_event_y'], axis=1, inplace=True, errors='ignore')\n",
    "    # Convert the timestamps in both dataframes to datetime format\n",
    "    df['timestamp'] = df['date-time']\n",
    "    #df_log['timestamp_l'] = df_log['Time_log']\n",
    "\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], format='%Y%m%d-%H%M%S%f')\n",
    "    #df_log['timestamp_l'] = pd.to_datetime(df_log['timestamp_l'], format='%Y%m%d-%H%M%S')\n",
    "\n",
    "    # Sort the dataframes by the timestamp\n",
    "    df = df.sort_values('timestamp')\n",
    "    df_log = df_log.sort_values('timestamp')\n",
    "\n",
    "    # Use merge_asof to merge the two dataframes, finding the nearest match on the timestamp\n",
    "    df_combined = pd.merge_asof(df, df_log, left_on='timestamp', right_on='timestamp', direction='backward')\n",
    "    df_combined.drop('timestamp', axis=1, inplace=True)\n",
    "    #df_combined.drop('timestamp', axis=1, inplace=True)\n",
    "\n",
    "    #print(df_combined.head())\n",
    "\n",
    "    # Update database\n",
    "    df_combined.to_sql(profile, engine, if_exists='replace', index=False)\n",
    "    print(\"Updated {}\".format(profile))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15                         TT     T1     T2     TH  relock  restart\n",
      "timestamp                                                       \n",
      "2022-05-18 21:07:34  27.97  27.75  28.19  27.81    True    False\n",
      "2022-05-18 21:16:25  26.94  26.38  27.50  26.88    True    False\n",
      "2022-05-18 21:18:47  25.91  25.19  26.63  25.69    True    False\n",
      "2022-05-18 21:20:55  24.88  24.00  25.75  24.50    True    False\n",
      "2022-05-18 21:23:03  23.84  22.88  24.81  23.38    True    False\n",
      "2022-05-18 21:25:12  22.78  21.75  23.81  22.19    True    False\n",
      "2022-05-18 21:27:21  21.75  20.69  22.81  21.06    True    False\n",
      "2022-05-18 21:29:33  20.72  19.63  21.81  20.00    True    False\n",
      "2022-05-18 21:46:14  20.16  17.81  22.50  19.44    True    False\n",
      "2022-05-18 22:02:56  20.06  17.44  22.69  19.19    True    False\n",
      "2022-05-18 22:19:38  20.00  17.19  22.81  19.13    True    False\n",
      "2022-05-18 22:36:19  20.09  17.19  23.00  20.31    True    False\n",
      "2022-05-18 22:53:01  20.19  17.38  23.00  19.69    True    False\n",
      "2022-05-18 23:09:43  20.13  18.00  22.25  19.31    True    False\n",
      "2022-05-18 23:26:24  21.06  20.75  21.38  21.13    True    False\n"
     ]
    }
   ],
   "source": [
    "restart_rows = df[df['relock'] == True]\n",
    "print(len(restart_rows), restart_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Object id (per Profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for profile in tqdm(table_names):\n",
    "    query = f'SELECT * FROM \"{profile}\"'\n",
    "    df = pd.read_sql_query(query, engine)\n",
    "    \n",
    "    df.drop(['obj_id'], axis=1, inplace=True, errors='ignore')\n",
    "    sorted_df = df.sort_values(by='filename')\n",
    "    print(sorted_df.head())\n",
    "    sorted_fn_list = sorted_df['filename'].tolist()\n",
    "    obj_ids = []\n",
    "    id_cnt = 0\n",
    "    for img in sorted_fn_list:\n",
    "        curr_id = id_cnt\n",
    "        obj_ids.append('obj_'+str(curr_id))\n",
    "        id_cnt = id_cnt+1\n",
    "    sorted_df['obj_id'] = obj_ids\n",
    "    sorted_df.to_sql(profile, engine, if_exists='replace', index=False)\n",
    "    print(\"Updated {}\".format(profile))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate UMAP Embeddings for selected \"handcrafted\" features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/veit/anaconda3/envs/pisco-dev/lib/python3.10/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UMAP(angular_rp_forest=True, metric='cosine', verbose=True)\n",
      "Tue Mar 25 14:52:25 2025 Construct fuzzy simplicial set\n",
      "Tue Mar 25 14:52:25 2025 Finding Nearest Neighbors\n",
      "Tue Mar 25 14:52:25 2025 Building RP forest with 64 trees\n",
      "Tue Mar 25 14:52:36 2025 NN descent for 21 iterations\n",
      "\t 1  /  21\n",
      "\t 2  /  21\n",
      "\tStopping threshold met -- exiting after 2 iterations\n",
      "Tue Mar 25 14:53:13 2025 Finished Nearest Neighbor Search\n",
      "Tue Mar 25 14:53:18 2025 Construct embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs completed:   2%| ▏          3/200 [00:00]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tcompleted  0  /  200 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs completed:  11%| █          22/200 [00:03]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tcompleted  20  /  200 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs completed:  21%| ██         42/200 [00:06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tcompleted  40  /  200 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs completed:  31%| ███        62/200 [00:08]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tcompleted  60  /  200 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs completed:  41%| ████       82/200 [00:11]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tcompleted  80  /  200 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs completed:  51%| █████      102/200 [00:14]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tcompleted  100  /  200 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs completed:  61%| ██████     122/200 [00:17]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tcompleted  120  /  200 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs completed:  71%| ███████    142/200 [00:19]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tcompleted  140  /  200 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs completed:  81%| ████████   162/200 [00:22]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tcompleted  160  /  200 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs completed:  91%| █████████  182/200 [00:25]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tcompleted  180  /  200 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs completed: 100%| ██████████ 200/200 [00:28]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Mar 25 15:10:29 2025 Finished embedding\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/media/veit/30781fe1-cea5-4503-ae00-1986beb935d2/Segmentation_results/M181/results_240328/standard_scaler.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 76\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Save the fitted scaler and reducer to disk\u001b[39;00m\n\u001b[1;32m     75\u001b[0m scaler_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstandard_scaler.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mscaler_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     77\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(scaler, f)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStandardScaler saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscaler_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pisco-dev/lib/python3.10/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/media/veit/30781fe1-cea5-4503-ae00-1986beb935d2/Segmentation_results/M181/results_240328/standard_scaler.pkl'"
     ]
    }
   ],
   "source": [
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "\n",
    "root_dir = '/media/veit/30781fe1-cea5-4503-ae00-1986beb935d2/Segmentation_results/M181/results_240328'\n",
    "\n",
    "selected_features = [\n",
    "       # 'pressure [dbar]', \n",
    "       # 'temperature', \n",
    "       # 'area', \n",
    "       # 'w', \n",
    "       # 'h', \n",
    "       'esd', \n",
    "       # 'interpolated_s', \n",
    "       # 'interpolated_o',\n",
    "       # 'interpolated_t', \n",
    "       # 'interpolated_chl', \n",
    "       'object_area_exc', \n",
    "       'object_area_rprops', \n",
    "       'object_%area',\n",
    "       'object_major_axis_len', \n",
    "       'object_minor_axis_len', \n",
    "       'object_centroid_y', \n",
    "       'object_centroid_x', \n",
    "       'object_convex_area',\n",
    "       'object_min_intensity', \n",
    "       'object_max_intensity', \n",
    "       'object_mean_intensity', \n",
    "       'object_int_density', \n",
    "       'object_perimeter',\n",
    "       'object_elongation', \n",
    "       'object_range', \n",
    "       'object_perim_area_excl', \n",
    "       'object_perim_major', \n",
    "       'object_circularity_area_excl', \n",
    "       'object_angle',\n",
    "       'object_boundbox_area', \n",
    "       'object_eccentricity', \n",
    "       'object_equivalent_diameter',\n",
    "       'object_euler_nr', \n",
    "       'object_extent', \n",
    "       'object_local_centroid_col', \n",
    "       'object_local_centroid_row',\n",
    "       'object_solidity', \n",
    "       'TAG_event', \n",
    "       'part_based_filter'\n",
    "]\n",
    "\n",
    "all_dfs = []  # list to store all the dataframes\n",
    "\n",
    "for profile in table_names:\n",
    "       query = f'SELECT * FROM \"{profile}\"'\n",
    "       df = pd.read_sql_query(query, engine)\n",
    "       df_selected = df[selected_features]\n",
    "       all_dfs.append(df_selected)  # append the selected dataframe to the list\n",
    "\n",
    "# combine all dataframes\n",
    "df_selected_comb = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "# #fit the UMAP model to the first profile\n",
    "# query = f'SELECT * FROM \"{table_names[0]}\"'\n",
    "# df = pd.read_sql_query(query, engine)\n",
    "# df_selected = df[selected_features]\n",
    "#print(df_selected.head())\n",
    "# Same for a StandardScaler instance\n",
    "scaler = StandardScaler().fit(df_selected_comb)\n",
    "df_selected_scaled = scaler.transform(df_selected_comb)\n",
    "#print(df_selected_scaled)\n",
    "reducer = umap.UMAP(metric='cosine', verbose=True).fit(df_selected_scaled)\n",
    "\n",
    "# Save the fitted scaler and reducer to disk\n",
    "scaler_path = os.path.join(root_dir, 'standard_scaler.pkl')\n",
    "with open(scaler_path, 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(f'StandardScaler saved to {scaler_path}')\n",
    "\n",
    "reducer_path = os.path.join(root_dir, 'umap_reducer.pkl')\n",
    "with open(reducer_path, 'wb') as f:\n",
    "    pickle.dump(reducer, f)\n",
    "print(f'UMAP reducer saved to {reducer_path}')\n",
    "\n",
    "for profile in table_names:\n",
    "       query = f'SELECT * FROM \"{profile}\"'\n",
    "       df = pd.read_sql_query(query, engine)\n",
    "       df_selected = df[selected_features]\n",
    "       \n",
    "       # transform the scaler to the selected features\n",
    "       df_selected_scaled = scaler.transform(df_selected)\n",
    "\n",
    "       # Then transform the UMAP model with the scaled data\n",
    "       embedding = reducer.transform(df_selected_scaled) \n",
    "\n",
    "       #plot embedding \n",
    "       plt.scatter(embedding[:, 0], embedding[:, 1])\n",
    "       plt.gca().set_aspect('equal', 'datalim')\n",
    "       plt.title('UMAP projection of the selected features', fontsize=18)\n",
    "       plt.show()\n",
    "       \n",
    "       #add embedding to database\n",
    "       df.drop(['umap_x', 'umap_y'], axis=1, inplace=True, errors='ignore')\n",
    "       df['umap_x']=embedding[:, 0]\n",
    "       df['umap_y']=embedding[:, 1]\n",
    "       df.to_sql(profile, engine, if_exists='replace', index=False)\n",
    "       print(\"Updated {}\".format(profile))\n",
    "\n",
    "       #save embedding to pickle file\n",
    "       # profile = profile.replace('deg', '°')  \n",
    "       # dest = os.path.join(root_dir, profile, 'handcrafted_embedding.pkl')\n",
    "       # with open(dest, 'wb') as f:\n",
    "       #        pickle.dump(embedding, f)\n",
    "       # print(f'UMAP embedding saved to {dest}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StandardScaler saved to /media/veit/30781fe1-cea5-4503-ae00-1986beb935d2/Segmentation_results/M181/results_240328/standard_scaler.pkl\n",
      "Tue Mar 25 15:13:00 2025 Worst tree score: 0.69144161\n",
      "Tue Mar 25 15:13:00 2025 Mean tree score: 0.69391879\n",
      "Tue Mar 25 15:13:00 2025 Best tree score: 0.69594535\n",
      "Tue Mar 25 15:13:05 2025 Forward diversification reduced edges from 26747790 to 12961216\n",
      "Tue Mar 25 15:13:06 2025 Reverse diversification reduced edges from 12961216 to 12961216\n",
      "Tue Mar 25 15:13:07 2025 Degree pruning reduced edges from 13973406 to 13973406\n",
      "Tue Mar 25 15:13:07 2025 Resorting data and graph based on tree order\n",
      "Tue Mar 25 15:13:07 2025 Building and compiling search function\n",
      "UMAP reducer saved to /media/veit/30781fe1-cea5-4503-ae00-1986beb935d2/Segmentation_results/M181/results_240328/umap_reducer.pkl\n"
     ]
    }
   ],
   "source": [
    "# Save the fitted scaler and reducer to disk\n",
    "scaler_path = os.path.join(root_dir, 'standard_scaler.pkl')\n",
    "with open(scaler_path, 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(f'StandardScaler saved to {scaler_path}')\n",
    "\n",
    "reducer_path = os.path.join(root_dir, 'umap_reducer.pkl')\n",
    "with open(reducer_path, 'wb') as f:\n",
    "    pickle.dump(reducer, f)\n",
    "print(f'UMAP reducer saved to {reducer_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add embedding to df afterwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated M181-245-1_CTD-065_00deg00S-031deg00W_20220514-0907\n",
      "Updated M181-200-1_CTD-055_00deg00S-023deg00W_20220511-0023\n",
      "Updated M181-227-1_CTD-060_00deg00S-027deg00W_20220512-1748\n",
      "Updated M181-126-1_CTD-040_00deg00S-010deg00W_20220505-1401\n",
      "Updated M181-107-1_CTD-036_00deg00S-007deg00W_20220504-0756\n",
      "Updated M181-175-1_CTD-050_00deg00S-019deg00W_20220509-0543\n",
      "Updated M181-267-1_CTD-070_00deg00S-035deg00W_20220516-0335\n",
      "Updated M181-140-1_CTD-043_00deg00S-013deg00W_20220506-1822\n",
      "Updated M181-160-1_CTD-047_00deg00S-016deg00W_20220508-0038\n",
      "Updated M181-285-1_CTD-075_00deg00S-039deg00W_20220517-2024\n",
      "Updated M181-295-1_CTD-080_00deg00S-041deg00W_20220518-2053\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "for profile in table_names:\n",
    "       query = f'SELECT * FROM \"{profile}\"'\n",
    "       df = pd.read_sql_query(query, engine)\n",
    "       features_loc = os.path.dirname(os.path.dirname(df['overview_path'].iloc[0])) + '/handcrafted_embedding.pkl'\n",
    "       with open(features_loc, 'rb') as f:\n",
    "              embedding = pickle.load(f)\n",
    "       df['umap_x']=embedding[:, 0]\n",
    "       df['umap_y']=embedding[:, 1]\n",
    "       df.to_sql(profile, engine, if_exists='replace', index=False)\n",
    "       print(\"Updated {}\".format(profile))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add prediction of schlieren images by filtering based on particle counts per image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated M181-160-1_CTD-047_00deg00S-016deg00W_20220508-0038\n",
      "Updated M181-245-1_CTD-065_00deg00S-031deg00W_20220514-0907\n",
      "Updated M181-200-1_CTD-055_00deg00S-023deg00W_20220511-0023\n",
      "Updated M181-227-1_CTD-060_00deg00S-027deg00W_20220512-1748\n",
      "Updated M181-126-1_CTD-040_00deg00S-010deg00W_20220505-1401\n",
      "Updated M181-107-1_CTD-036_00deg00S-007deg00W_20220504-0756\n",
      "Updated M181-175-1_CTD-050_00deg00S-019deg00W_20220509-0543\n",
      "Updated M181-267-1_CTD-070_00deg00S-035deg00W_20220516-0335\n",
      "Updated M181-140-1_CTD-043_00deg00S-013deg00W_20220506-1822\n",
      "Updated M181-285-1_CTD-075_00deg00S-039deg00W_20220517-2024\n",
      "Updated M181-295-1_CTD-080_00deg00S-041deg00W_20220518-2053\n"
     ]
    }
   ],
   "source": [
    "for profile in table_names:\n",
    "    query = f'SELECT * FROM \"{profile}\"'\n",
    "    df = pd.read_sql_query(query, engine)\n",
    "    df.drop(['part_based_filter'], axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "    df_unique = df[['date-time', 'pressure [dbar]', 'depth [m]', 'img_id','temperature','overview_path','interpolated_s','interpolated_t','interpolated_o','interpolated_chl','interpolated_z_factor','restart','relock','TAG_event']].drop_duplicates()\n",
    "    df_count = df.groupby('date-time').size().reset_index(name='count')\n",
    "    df_unique = df_unique.merge(df_count, on='date-time', how='left')\n",
    "    df_unique = df_unique.sort_values('pressure [dbar]')\n",
    "\n",
    "\n",
    "    # Filter the data\n",
    "    df_unique['part_based_filter'] = df_unique['count'].apply(lambda x: 0 if x < df_unique['count'].std() else 1)\n",
    "    #print(df_unique.head())\n",
    "    filtered_df = df_unique[df_unique['part_based_filter'] == 0]\n",
    "\n",
    "    # Merge 'df_unique' back to 'df' to create 'part_based_filter' column in 'df'\n",
    "    df = df.merge(df_unique[['date-time', 'part_based_filter']], on='date-time', how='left')\n",
    "    df.to_sql(profile, engine, if_exists='replace', index=False)\n",
    "    print(df['part_based_filter'].value_counts())\n",
    "    print(\"Updated {}\".format(profile))\n",
    "    #print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           date-time  pressure [dbar]  depth [m]  temperature  index  img_id  \\\n",
      "0  20220506-18475929            -0.08      -0.08        32.27   1005       1   \n",
      "1  20220506-18475929            -0.08      -0.08        32.27   1012       1   \n",
      "2  20220506-18475929            -0.08      -0.08        32.27    103       1   \n",
      "3  20220506-18475929            -0.08      -0.08        32.27   1040       1   \n",
      "4  20220506-18475929            -0.08      -0.08        32.27    105       1   \n",
      "\n",
      "                                       filename    area       x       y  ...  \\\n",
      "0  20220506-18475929_000.992bar_32.27C_1005.png  9945.0  2102.5   266.5  ...   \n",
      "1  20220506-18475929_000.992bar_32.27C_1012.png   459.5   508.5   202.5  ...   \n",
      "2   20220506-18475929_000.992bar_32.27C_103.png   508.5  2476.5  2105.5  ...   \n",
      "3  20220506-18475929_000.992bar_32.27C_1040.png   690.0   362.0   136.0  ...   \n",
      "4   20220506-18475929_000.992bar_32.27C_105.png   440.0   445.5  2097.5  ...   \n",
      "\n",
      "      TT     T1     T2     TH  relock restart  obj_id     umap_x     umap_y  \\\n",
      "0  32.72  32.38  33.06  32.44   False   False   obj_0   1.634831  -5.788402   \n",
      "1  32.72  32.38  33.06  32.44   False   False   obj_1  12.564866  -5.910573   \n",
      "2  32.72  32.38  33.06  32.44   False   False   obj_2  14.058960  -0.872380   \n",
      "3  32.72  32.38  33.06  32.44   False   False   obj_3  15.630560  10.468481   \n",
      "4  32.72  32.38  33.06  32.44   False   False   obj_4   8.701097  -4.627170   \n",
      "\n",
      "   part_based_filter  \n",
      "0                  1  \n",
      "1                  1  \n",
      "2                  1  \n",
      "3                  1  \n",
      "4                  1  \n",
      "\n",
      "[5 rows x 64 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df['part_based_filter'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export images to a directory and zip it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded images from M181-175-1_CTD-050_00deg00S-019deg00W_20220509-0543\n",
      "Loaded images from M181-267-1_CTD-070_00deg00S-035deg00W_20220516-0335\n",
      "Loaded images from M181-140-1_CTD-043_00deg00S-013deg00W_20220506-1822\n",
      "Loaded images from M181-160-1_CTD-047_00deg00S-016deg00W_20220508-0038\n",
      "Loaded images from M181-245-1_CTD-065_00deg00S-031deg00W_20220514-0907\n",
      "Loaded images from M181-200-1_CTD-055_00deg00S-023deg00W_20220511-0023\n",
      "Loaded images from M181-227-1_CTD-060_00deg00S-027deg00W_20220512-1748\n",
      "Loaded images from M181-126-1_CTD-040_00deg00S-010deg00W_20220505-1401\n",
      "Loaded images from M181-285-1_CTD-075_00deg00S-039deg00W_20220517-2024\n",
      "Loaded images from M181-295-1_CTD-080_00deg00S-041deg00W_20220518-2053\n",
      "Loaded images from M181-107-1_CTD-036_00deg00S-007deg00W_20220504-0756\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from PIL import Image\n",
    "\n",
    "# create a new directory for the images\n",
    "os.makedirs('/media/plankton/30781fe1-cea5-4503-ae00-1986beb935d2/exported_images', exist_ok=True)\n",
    "df_all = pd.DataFrame()\n",
    "\n",
    "for profile in table_names:\n",
    "    query = f'SELECT * FROM \"{profile}\"'\n",
    "    df = pd.read_sql_query(query, engine)\n",
    "    # go through the DataFrame\n",
    "    df_filtered = df[(df['TAG_event'] == 0) & (df['part_based_filter'] == 0)]\n",
    "    # for idx, row in df_filtered.iterrows():\n",
    "    #     # read the image\n",
    "    #     img = Image.open(row['full_path'])\n",
    "    #     # save to the new location\n",
    "    #     img.save(os.path.join('/media/plankton/30781fe1-cea5-4503-ae00-1986beb935d2/exported_images', os.path.basename(row['full_path'])))\n",
    "    print(\"Loaded images from {}\".format(profile))\n",
    "    df_all = pd.concat([df_all, df_filtered])\n",
    "\n",
    "# save df_all to a csv file\n",
    "df_all.to_csv('/media/plankton/30781fe1-cea5-4503-ae00-1986beb935d2/exported_images.csv')\n",
    "\n",
    "# make a zip file\n",
    "#shutil.make_archive('/media/plankton/30781fe1-cea5-4503-ae00-1986beb935d2/exported_images', 'zip', '/media/plankton/30781fe1-cea5-4503-ae00-1986beb935d2/exported_images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract crops from raw images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded images from M181-175-1_CTD-050_00deg00S-019deg00W_20220509-0543\n",
      "Loaded images from M181-267-1_CTD-070_00deg00S-035deg00W_20220516-0335\n",
      "Loaded images from M181-140-1_CTD-043_00deg00S-013deg00W_20220506-1822\n",
      "Loaded images from M181-160-1_CTD-047_00deg00S-016deg00W_20220508-0038\n",
      "Loaded images from M181-245-1_CTD-065_00deg00S-031deg00W_20220514-0907\n",
      "Loaded images from M181-200-1_CTD-055_00deg00S-023deg00W_20220511-0023\n",
      "Loaded images from M181-227-1_CTD-060_00deg00S-027deg00W_20220512-1748\n",
      "Loaded images from M181-126-1_CTD-040_00deg00S-010deg00W_20220505-1401\n",
      "Loaded images from M181-285-1_CTD-075_00deg00S-039deg00W_20220517-2024\n",
      "Loaded images from M181-295-1_CTD-080_00deg00S-041deg00W_20220518-2053\n",
      "Loaded images from M181-107-1_CTD-036_00deg00S-007deg00W_20220504-0756\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from PIL import Image\n",
    "\n",
    "raw_root = '/mnt/m181'\n",
    "\n",
    "# scale the coordinates and the size of the crop\n",
    "scale_factor = 5120 / 2560  # target_resolution / original_resolution\n",
    "\n",
    "for profile in table_names:\n",
    "    image_name_old = ''\n",
    "    query = f'SELECT * FROM \"{profile}\"'\n",
    "    df = pd.read_sql_query(query, engine)\n",
    "    # go through the DataFrame\n",
    "    df_filtered = df[(df['TAG_event'] == 0) & (df['part_based_filter'] == 0)]\n",
    "    for idx, row in df_filtered.iterrows():\n",
    "        # read the image\n",
    "        img_name = os.path.basename(row['overview_path'])\n",
    "        if img_name != image_name_old:            \n",
    "            raw_img_path = os.path.join(raw_root, profile, 'PNG', img_name)\n",
    "            img = Image.open(raw_img_path)\n",
    "            image_name_old = img_name\n",
    "        \n",
    "        x = row['x']\n",
    "        y = row['y']\n",
    "        w = row['w']\n",
    "        h = row['h']\n",
    "\n",
    "        \n",
    "        x = x * scale_factor\n",
    "        y = y * scale_factor\n",
    "        w = w * scale_factor\n",
    "        h = h * scale_factor\n",
    "\n",
    "        # calculate the top left corner of the crop\n",
    "        x1 = x - w / 2\n",
    "        y1 = y - h / 2\n",
    "\n",
    "        # calculate the bottom right corner of the crop\n",
    "        x2 = x + w / 2\n",
    "        y2 = y + h / 2\n",
    "\n",
    "        # crop the image\n",
    "        cropped_image = img.crop((x1, y1, x2, y2))\n",
    "\n",
    "        # save the cropped image\n",
    "        cropped_image.save(os.path.join('/media/plankton/30781fe1-cea5-4503-ae00-1986beb935d2/exported_images_raw', os.path.basename(row['full_path'])))\n",
    "    print(\"Loaded images from {}\".format(profile))\n",
    "shutil.make_archive('/media/plankton/30781fe1-cea5-4503-ae00-1986beb935d2/exported_images_raw', 'zip', '/media/plankton/30781fe1-cea5-4503-ae00-1986beb935d2/exported_images_raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export binary masks to zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded images from M181-175-1_CTD-050_00deg00S-019deg00W_20220509-0543\n",
      "Loaded images from M181-267-1_CTD-070_00deg00S-035deg00W_20220516-0335\n",
      "Loaded images from M181-140-1_CTD-043_00deg00S-013deg00W_20220506-1822\n",
      "Loaded images from M181-160-1_CTD-047_00deg00S-016deg00W_20220508-0038\n",
      "Loaded images from M181-245-1_CTD-065_00deg00S-031deg00W_20220514-0907\n",
      "Loaded images from M181-200-1_CTD-055_00deg00S-023deg00W_20220511-0023\n",
      "Loaded images from M181-227-1_CTD-060_00deg00S-027deg00W_20220512-1748\n",
      "Loaded images from M181-126-1_CTD-040_00deg00S-010deg00W_20220505-1401\n",
      "Loaded images from M181-285-1_CTD-075_00deg00S-039deg00W_20220517-2024\n",
      "Loaded images from M181-295-1_CTD-080_00deg00S-041deg00W_20220518-2053\n",
      "Loaded images from M181-107-1_CTD-036_00deg00S-007deg00W_20220504-0756\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/media/plankton/30781fe1-cea5-4503-ae00-1986beb935d2/exported_images_masks.zip'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from PIL import Image\n",
    "\n",
    "# create a new directory for the images\n",
    "os.makedirs('/media/plankton/30781fe1-cea5-4503-ae00-1986beb935d2/exported_images_masks', exist_ok=True)\n",
    "df_all = pd.DataFrame()\n",
    "\n",
    "for profile in table_names:\n",
    "    query = f'SELECT * FROM \"{profile}\"'\n",
    "    df = pd.read_sql_query(query, engine)\n",
    "    # go through the DataFrame\n",
    "    df_filtered = df[(df['TAG_event'] == 0) & (df['part_based_filter'] == 0)]\n",
    "    for idx, row in df_filtered.iterrows():\n",
    "        mask_name = os.path.basename(row['full_path'])\n",
    "        mask_name = mask_name[:-4] + '_mask.png'\n",
    "        mask_path = os.path.join(os.path.dirname(os.path.dirname(row['full_path'])), 'Masks', mask_name)\n",
    "        # read the image\n",
    "        img = Image.open(mask_path)\n",
    "        # save to the new location\n",
    "        img.save(os.path.join('/media/plankton/30781fe1-cea5-4503-ae00-1986beb935d2/exported_images_masks', mask_name))\n",
    "    print(\"Loaded images from {}\".format(profile))\n",
    "    df_all = pd.concat([df_all, df_filtered])\n",
    "\n",
    "# save df_all to a csv file\n",
    "df_all.to_csv('/media/plankton/30781fe1-cea5-4503-ae00-1986beb935d2/exported_images.csv')\n",
    "\n",
    "# make a zip file\n",
    "shutil.make_archive('/media/plankton/30781fe1-cea5-4503-ae00-1986beb935d2/exported_images_masks', 'zip', '/media/plankton/30781fe1-cea5-4503-ae00-1986beb935d2/exported_images_masks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.dispose()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pisco-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
