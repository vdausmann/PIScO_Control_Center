{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pandas.errors import EmptyDataError\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import text\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup Database\n",
    "#Setup connection parameters\n",
    "username = 'plankton'\n",
    "password = 'piscodisco'\n",
    "host = 'localhost'  # or the IP address of your database server\n",
    "port = '5432'       # default port for PostgreSQL\n",
    "database = 'pisco_crops_db'\n",
    "\n",
    "# Create an engine that connects to the PostgreSQL server\n",
    "engine = create_engine(f'postgresql://{username}:{password}@{host}:{port}/{database}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['M181-175-1_CTD-050_00deg00S-019deg00W_20220509-0543', 'M181-267-1_CTD-070_00deg00S-035deg00W_20220516-0335', 'M181-140-1_CTD-043_00deg00S-013deg00W_20220506-1822', 'M181-160-1_CTD-047_00deg00S-016deg00W_20220508-0038', 'M181-245-1_CTD-065_00deg00S-031deg00W_20220514-0907', 'M181-200-1_CTD-055_00deg00S-023deg00W_20220511-0023', 'M181-227-1_CTD-060_00deg00S-027deg00W_20220512-1748', 'M181-126-1_CTD-040_00deg00S-010deg00W_20220505-1401', 'M181-285-1_CTD-075_00deg00S-039deg00W_20220517-2024', 'M181-295-1_CTD-080_00deg00S-041deg00W_20220518-2053', 'M181-107-1_CTD-036_00deg00S-007deg00W_20220504-0756']\n",
      "11 entries found\n"
     ]
    }
   ],
   "source": [
    "# Fetch all table names in the database\n",
    "query_tables = \"SELECT tablename FROM pg_tables WHERE schemaname='public'\"\n",
    "with engine.connect() as conn:\n",
    "    table_names = pd.read_sql_query(text(query_tables), conn)['tablename'].tolist()\n",
    "print(table_names)\n",
    "print(len(table_names),'entries found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M181-245-1_CTD-065_00deg00S-031deg00W_20220514-0907 130670\n",
      "130670 61\n",
      "0\n",
      "M181-200-1_CTD-055_00deg00S-023deg00W_20220511-0023 117511\n",
      "117511 61\n",
      "0\n",
      "M181-227-1_CTD-060_00deg00S-027deg00W_20220512-1748 112349\n",
      "112349 61\n",
      "0\n",
      "M181-126-1_CTD-040_00deg00S-010deg00W_20220505-1401 117337\n",
      "117337 61\n",
      "0\n",
      "M181-107-1_CTD-036_00deg00S-007deg00W_20220504-0756 196602\n",
      "196602 61\n",
      "0\n",
      "M181-175-1_CTD-050_00deg00S-019deg00W_20220509-0543 105195\n",
      "105195 61\n",
      "0\n",
      "M181-267-1_CTD-070_00deg00S-035deg00W_20220516-0335 78602\n",
      "78602 61\n",
      "0\n",
      "M181-140-1_CTD-043_00deg00S-013deg00W_20220506-1822 71463\n",
      "71463 61\n",
      "0\n",
      "M181-160-1_CTD-047_00deg00S-016deg00W_20220508-0038 104967\n",
      "104967 61\n",
      "0\n",
      "M181-285-1_CTD-075_00deg00S-039deg00W_20220517-2024 46623\n",
      "46623 61\n",
      "0\n",
      "M181-295-1_CTD-080_00deg00S-041deg00W_20220518-2053 92734\n",
      "92734 61\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for table_name in table_names:\n",
    "    query = f'SELECT * FROM \"{table_name}\"'\n",
    "    df = pd.read_sql_query(query, engine)\n",
    "    print(table_name, len(df['obj_id']))\n",
    "    print(len(df.index), len(df.columns))\n",
    "    #print(df.columns)\n",
    "    # assuming df is your DataFrame and 'col' is the column you want to count duplicates in\n",
    "    duplicates = df.duplicated(subset='obj_id')\n",
    "\n",
    "    # count the duplicates\n",
    "    num_duplicates = duplicates.sum()\n",
    "\n",
    "    print(num_duplicates)\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['date-time', 'pressure [dbar]', 'depth [m]', 'temperature', 'index',\n",
      "       'img_id', 'filename', 'area', 'x', 'y', 'w', 'h', 'saved', 'full_path',\n",
      "       'esd', 'overview_path', 'interpolated_s', 'interpolated_o',\n",
      "       'interpolated_t', 'interpolated_chl', 'interpolated_z_factor', 'width',\n",
      "       'height', 'bx', 'by', 'circ.', 'area_exc', 'area_rprops', '%area',\n",
      "       'major', 'minor', 'centroid_y', 'centroid_x', 'convex_area',\n",
      "       'min_intensity', 'max_intensity', 'mean_intensity', 'intden', 'perim.',\n",
      "       'elongation', 'range', 'perimareaexc', 'perimmajor', 'circex', 'angle',\n",
      "       'bounding_box_area', 'eccentricity', 'equivalent_diameter',\n",
      "       'euler_number', 'extent', 'local_centroid_col', 'local_centroid_row',\n",
      "       'solidity', 'TAG_event', 'TT', 'T1', 'T2', 'TH', 'relock', 'restart',\n",
      "       'obj_id', 'part_based_filter', 'umap_x', 'umap_y'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "query = f'SELECT * FROM \"M181-140-1_CTD-043_00deg00S-013deg00W_20220506-1822\"'\n",
    "df = pd.read_sql_query(query, engine)\n",
    "\n",
    "#print(len(df['date-time'].drop_duplicates()))\n",
    "\n",
    "print(df.columns)\n",
    "#df['overview_path'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique = df[['date-time', 'pressure [dbar]', 'depth [m]', 'img_id','temperature','overview_path','interpolated_s','interpolated_t','interpolated_o','interpolated_chl','interpolated_z_factor','restart','relock','TAG_event']].drop_duplicates()\n",
    "#df_unique = df[['date-time']].drop_duplicates()\n",
    "#df_count = df.groupby('date-time').size().reset_index(name='count')\n",
    "#df_unique = df_unique.merge(df_count, on='date-time', how='left')\n",
    "#df_unique = df_unique.sort_values('pressure [dbar]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7759"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum value in \"esd\" column: 609.71\n",
      "Corresponding value in \"area\" column: 400.5\n"
     ]
    }
   ],
   "source": [
    "# Find the index of the minimum value in 'esd' column\n",
    "index = df['esd'].idxmin()\n",
    "\n",
    "# Use .iloc[] to get the corresponding value in the 'area' column for the minimum index\n",
    "min_value_area = df.iloc[index]['area']\n",
    "\n",
    "print('Minimum value in \"esd\" column:', df.iloc[index]['esd'])\n",
    "print('Corresponding value in \"area\" column:', min_value_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           date-time  pressure [dbar]  depth [m]  temperature  index  img_id  \\\n",
      "0  20220505-14191884            -0.32     -0.318        28.51     17       1   \n",
      "1  20220505-14191884            -0.32     -0.318        28.51     73       1   \n",
      "2  20220505-14191884            -0.32     -0.318        28.51     93       1   \n",
      "3  20220505-14191884            -0.32     -0.318        28.51    102       1   \n",
      "4  20220505-14191884            -0.32     -0.318        28.51    106       1   \n",
      "\n",
      "                                      filename     area       x       y  ...  \\\n",
      "0   20220505-14191884_000.968bar_28.51C_17.png   1556.0   432.5  2514.5  ...   \n",
      "1   20220505-14191884_000.968bar_28.51C_73.png    547.5  2166.0  2325.0  ...   \n",
      "2   20220505-14191884_000.968bar_28.51C_93.png    738.5  2095.0  2286.5  ...   \n",
      "3  20220505-14191884_000.968bar_28.51C_102.png    501.5   663.0  2276.0  ...   \n",
      "4  20220505-14191884_000.968bar_28.51C_106.png  14396.5   414.0  2325.0  ...   \n",
      "\n",
      "        angle  bounding_box_area  eccentricity equivalent_diameter  \\\n",
      "0  146.316481             4345.0      0.959131           45.765479   \n",
      "1  161.838256             2016.0      0.954410           28.232038   \n",
      "2  170.915404             2162.0      0.989125           32.800723   \n",
      "3  177.827410              884.0      0.623904           26.558891   \n",
      "4   38.155800            20448.0      0.679303          136.347243   \n",
      "\n",
      "   euler_number    extent  local_centroid_col  local_centroid_row  solidity  \\\n",
      "0           1.0  0.378596           44.913070           22.085714  0.961426   \n",
      "1           1.0  0.310516           25.388179           18.939297  0.554473   \n",
      "2           1.0  0.390842           44.953846            8.415385  0.708893   \n",
      "3           1.0  0.626697           16.846570           12.456679  0.860248   \n",
      "4           1.0  0.714055           73.879529           69.681118  0.971134   \n",
      "\n",
      "                                       overview_path  \n",
      "0  /media/plankton/30781fe1-cea5-4503-ae00-1986be...  \n",
      "1  /media/plankton/30781fe1-cea5-4503-ae00-1986be...  \n",
      "2  /media/plankton/30781fe1-cea5-4503-ae00-1986be...  \n",
      "3  /media/plankton/30781fe1-cea5-4503-ae00-1986be...  \n",
      "4  /media/plankton/30781fe1-cea5-4503-ae00-1986be...  \n",
      "\n",
      "[5 rows x 53 columns]\n"
     ]
    }
   ],
   "source": [
    "def modify_full_path(path):\n",
    "    dirname, base_name = os.path.split(path)\n",
    "    base_parts = base_name.split('_')\n",
    "    new_base_name = '_'.join(base_parts[:-1]) + '.png'\n",
    "    return os.path.join(dirname.replace('Crops', 'Images'), new_base_name)\n",
    "\n",
    "df['overview_path'] = df['full_path'].apply(modify_full_path)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64                date-time  pressure [dbar]  depth [m]  img_id  temperature  \\\n",
      "11770  20220518-21073407             3.27      3.250    1102        28.88   \n",
      "11792  20220518-21073432             3.29      3.270    1103        28.88   \n",
      "11810  20220518-21073457             3.29      3.270    1104        28.88   \n",
      "11826  20220518-21073482             3.27      3.250    1105        28.90   \n",
      "80703  20220518-21162510           413.42    410.736    3226         9.47   \n",
      "...                  ...              ...        ...     ...          ...   \n",
      "90801  20220518-22025680          3433.15   3386.777   14391         2.47   \n",
      "90808  20220518-22025705          3433.33   3386.953   14392         2.47   \n",
      "90816  20220518-22025730          3433.56   3387.182   14393         2.47   \n",
      "90835  20220518-22025755          3433.76   3387.380   14394         2.47   \n",
      "90857  20220518-22025780          3434.01   3387.620   14395         2.47   \n",
      "\n",
      "                                           overview_path  interpolated_s  \\\n",
      "11770  /media/plankton/30781fe1-cea5-4503-ae00-1986be...       35.664473   \n",
      "11792  /media/plankton/30781fe1-cea5-4503-ae00-1986be...       35.664471   \n",
      "11810  /media/plankton/30781fe1-cea5-4503-ae00-1986be...       35.664471   \n",
      "11826  /media/plankton/30781fe1-cea5-4503-ae00-1986be...       35.664473   \n",
      "80703  /media/plankton/30781fe1-cea5-4503-ae00-1986be...       34.628202   \n",
      "...                                                  ...             ...   \n",
      "90801  /media/plankton/30781fe1-cea5-4503-ae00-1986be...       34.912130   \n",
      "90808  /media/plankton/30781fe1-cea5-4503-ae00-1986be...       34.912166   \n",
      "90816  /media/plankton/30781fe1-cea5-4503-ae00-1986be...       34.912100   \n",
      "90835  /media/plankton/30781fe1-cea5-4503-ae00-1986be...       34.912100   \n",
      "90857  /media/plankton/30781fe1-cea5-4503-ae00-1986be...       34.912299   \n",
      "\n",
      "       interpolated_t  interpolated_o  interpolated_chl  \\\n",
      "11770       28.769562        198.4515          -0.01410   \n",
      "11792       28.769574        198.4005          -0.01670   \n",
      "11810       28.769574        198.4005          -0.01670   \n",
      "11826       28.769562        198.4515          -0.01410   \n",
      "80703        7.424378        148.4344          -0.13984   \n",
      "...               ...             ...               ...   \n",
      "90801        2.539670        258.3620          -0.13345   \n",
      "90808        2.539634        258.2684          -0.13399   \n",
      "90816        2.539700        258.4400          -0.13300   \n",
      "90835        2.539700        258.4400          -0.13300   \n",
      "90857        2.539502        257.9278          -0.13621   \n",
      "\n",
      "       interpolated_z_factor  restart  relock TAG_event  \n",
      "11770               0.993783    False    True         1  \n",
      "11792               0.993817    False    True         1  \n",
      "11810               0.993817    False    True         1  \n",
      "11826               0.993783    False    True         1  \n",
      "80703               0.993507    False    True         2  \n",
      "...                      ...      ...     ...       ...  \n",
      "90801               0.986493    False    True        10  \n",
      "90808               0.986492    False    True        10  \n",
      "90816               0.986493    False    True        10  \n",
      "90835               0.986493    False    True        10  \n",
      "90857               0.986491    False    True        10  \n",
      "\n",
      "[64 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "restart_rows = df_unique[df_unique['relock'] == True]\n",
    "print(len(restart_rows), restart_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [05:04<00:00, 27.65s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for table_name in tqdm(table_names):\n",
    "    query = f'SELECT * FROM \"{table_name}\"'\n",
    "    df = pd.read_sql_query(query, engine)\n",
    "    df['overview_path'] = df['full_path'].apply(modify_full_path)\n",
    "    df.to_sql(table_name, engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add log information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to reformat timestamp\n",
    "# def reformat_timestamp(timestamp):\n",
    "#     # Format: YYYYMMDD_HHh_MMm_SSs to YYYYMMDD-HHMMSS\n",
    "#     formatted_timestamp = re.sub(r'(\\d{4})(\\d{2})(\\d{2})_(\\d{2})h_(\\d{2})m_(\\d{2})s', r'\\1\\2\\3-\\4\\5\\6', timestamp)\n",
    "#     return formatted_timestamp\n",
    "\n",
    "# def parse_line(line, row):\n",
    "#     if line.startswith(\"b'TT\"):\n",
    "#         temp_values = line[2:].rstrip(\"'\").split('_')\n",
    "#         row['TT']= float(temp_values[1])\n",
    "#         row['T1']= float(temp_values[3])\n",
    "#         row['T2']= float(temp_values[5])\n",
    "#         row['TH']= float(temp_values[7])\n",
    "\n",
    "#     # # elif line.startswith(\"b'Heat\"):\n",
    "#     # #     return {}\n",
    "#     # # elif line.startswith(\"b'Cool\"):\n",
    "#     # #     return {}\n",
    "#     # else:\n",
    "#     #     return {}\n",
    "\n",
    "# def create_log_df(file_path):\n",
    "\n",
    "#     # Read the log file\n",
    "#     with open(file_path, 'r') as f:\n",
    "#         lines = f.readlines()\n",
    "\n",
    "#     data = []\n",
    "#     temp_data = {}\n",
    "\n",
    "#     # Iterate through lines\n",
    "#     for line in lines:\n",
    "#         line = line.rstrip('\\n')\n",
    "#         # Check if line is a timestamp\n",
    "#         if \"h\" in line and \"m\" in line and \"s\" in line:\n",
    "#             if temp_data:\n",
    "#                 # If current timestamp already exists in data, update the existing dictionary\n",
    "#                 if any(d['timestamp'] == temp_data['timestamp'] for d in data):\n",
    "#                     existing_data = [d for d in data if d['timestamp'] == temp_data['timestamp']][0]\n",
    "#                     existing_data.update(temp_data)\n",
    "#                 else:\n",
    "#                     data.append(temp_data)\n",
    "#                 temp_data = {}\n",
    "#             temp_data['timestamp'] = line\n",
    "#         else:\n",
    "#             # Parse line according to message type\n",
    "#             if line.startswith(\"b'TT\"):\n",
    "#                 temp_values = line[2:].rstrip(\"'\").split('_')\n",
    "#                 temp_data['TT'] = float(temp_values[1])\n",
    "#                 temp_data['T1'] = float(temp_values[3])\n",
    "#                 temp_data['T2'] = float(temp_values[5])\n",
    "#                 temp_data['TH'] = float(temp_values[7])\n",
    "#             elif line.startswith('Restart Tag'):\n",
    "#                 temp_data['restart'] = True\n",
    "#             elif line == 'Relock':\n",
    "#                 temp_data['relock'] = True\n",
    "\n",
    "#     # If there is data waiting after the last line, add it\n",
    "#     if temp_data:\n",
    "#         if any(d['timestamp'] == temp_data['timestamp'] for d in data):\n",
    "#             existing_data = [d for d in data if d['timestamp'] == temp_data['timestamp']][0]\n",
    "#             existing_data.update(temp_data)\n",
    "#         else:\n",
    "#             data.append(temp_data)\n",
    "\n",
    "#     # Create a dataframe from the data list\n",
    "#     df = pd.DataFrame(data)\n",
    "\n",
    "#     # Convert timestamps to datetime and set it as index\n",
    "#     df['timestamp'] = pd.to_datetime(df['timestamp'], format=\"%Y%m%d_%Hh_%Mm_%Ss\")\n",
    "#     df.set_index('timestamp', inplace=True)\n",
    "\n",
    "#     # Replace NaN values in 'relock' and 'restart' columns with False\n",
    "#     df[['restart', 'relock']] = df[['restart', 'relock']].fillna(False)\n",
    "#     cols = ['TT', 'T1', 'T2', 'TH']  # list of your column names\n",
    "#     for col in cols:\n",
    "#         df[col] = df[col].interpolate()\n",
    "    \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "## new create log function\n",
    "def create_log_df(file_path):\n",
    "    # Read the log file\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    data = []\n",
    "    temp_data = {}\n",
    "    indicator = 0\n",
    "    counter = 1\n",
    "\n",
    "    # Iterate through lines\n",
    "    for line in lines:\n",
    "        line = line.rstrip('\\n')\n",
    "        #print(line)\n",
    "        # Check if line is a timestamp\n",
    "        if \"h\" in line and \"m\" in line and \"s\" in line:\n",
    "            if temp_data and 'timestamp' in temp_data:\n",
    "                # If current timestamp already exists in data, update the existing dictionary\n",
    "                if any(d['timestamp'] == temp_data['timestamp'] for d in data):\n",
    "                    existing_data = [d for d in data if d['timestamp'] == temp_data['timestamp']][0]\n",
    "                    existing_data.update(temp_data)\n",
    "                else:\n",
    "                    data.append(temp_data)\n",
    "                temp_data = {}\n",
    "            temp_data['timestamp'] = line\n",
    "        else:\n",
    "            # Parse line according to message type\n",
    "            if line.startswith(\"b'TT\"):\n",
    "                temp_values = line[2:].rstrip(\"'\").split('_')\n",
    "                temp_data['TT'] = float(temp_values[1])\n",
    "                temp_data['T1'] = float(temp_values[3])\n",
    "                temp_data['T2'] = float(temp_values[5])\n",
    "                temp_data['TH'] = float(temp_values[7])\n",
    "            elif line.startswith('Restart Tag'):\n",
    "                temp_data['restart'] = True\n",
    "                indicator = 0\n",
    "            elif line == 'Relock':\n",
    "                temp_data['relock'] = True\n",
    "                indicator = counter\n",
    "                counter += 1\n",
    "            temp_data['TAG_event'] = indicator\n",
    "\n",
    "    # If there is data waiting after the last line, add it\n",
    "    if temp_data:\n",
    "        if any(d['timestamp'] == temp_data['timestamp'] for d in data):\n",
    "            existing_data = [d for d in data if d['timestamp'] == temp_data['timestamp']][0]\n",
    "            existing_data.update(temp_data)\n",
    "        else:\n",
    "            data.append(temp_data)\n",
    "\n",
    "    # Create a dataframe from the data list\n",
    "    df = pd.DataFrame(data)\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], format=\"%Y%m%d_%Hh_%Mm_%Ss\")\n",
    "    df.set_index('timestamp', inplace=True)\n",
    "\n",
    "    # Replace NaN values in 'relock' and 'restart' columns with False\n",
    "    df[['restart', 'relock']] = df[['restart', 'relock']].fillna(False)\n",
    "    cols = ['TT', 'T1', 'T2', 'TH']  # list of your column names\n",
    "    for col in cols:\n",
    "        df[col] = df[col].interpolate()\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closest logfile: 20220512_17h_47m__Templog.txt, Size: 509203 bytes\n",
      "Updated M181-227-1_CTD-060_00deg00S-027deg00W_20220512-1748\n",
      "Closest logfile: 20220505_14h_01m__Templog.txt, Size: 821675 bytes\n",
      "Updated M181-126-1_CTD-040_00deg00S-010deg00W_20220505-1401\n",
      "Closest logfile: 20220504_07h_56m__Templog.txt, Size: 846382 bytes\n",
      "Updated M181-107-1_CTD-036_00deg00S-007deg00W_20220504-0756\n",
      "Closest logfile: 20220509_05h_43m__Templog.txt, Size: 717942 bytes\n",
      "Updated M181-175-1_CTD-050_00deg00S-019deg00W_20220509-0543\n",
      "Closest logfile: 20220516_03h_34m__Templog.txt, Size: 761965 bytes\n",
      "Updated M181-267-1_CTD-070_00deg00S-035deg00W_20220516-0335\n",
      "Closest logfile: 20220506_18h_22m__Templog.txt, Size: 772615 bytes\n",
      "Updated M181-140-1_CTD-043_00deg00S-013deg00W_20220506-1822\n",
      "Closest logfile: 20220508_00h_38m__Templog.txt, Size: 568372 bytes\n",
      "Updated M181-160-1_CTD-047_00deg00S-016deg00W_20220508-0038\n",
      "Closest logfile: 20220517_20h_24m__Templog.txt, Size: 627455 bytes\n",
      "Updated M181-285-1_CTD-075_00deg00S-039deg00W_20220517-2024\n",
      "Closest logfile: 20220518_20h_52m__Templog.txt, Size: 601286 bytes\n",
      "Updated M181-295-1_CTD-080_00deg00S-041deg00W_20220518-2053\n",
      "Closest logfile: 20220514_09h_06m__Templog.txt, Size: 735741 bytes\n",
      "Updated M181-245-1_CTD-065_00deg00S-031deg00W_20220514-0907\n",
      "Closest logfile: 20220511_00h_23m__Templog.txt, Size: 668470 bytes\n",
      "Updated M181-200-1_CTD-055_00deg00S-023deg00W_20220511-0023\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "directory = '/media/plankton/30781fe1-cea5-4503-ae00-1986beb935d2/Segmentation_results/M181/Templog/'\n",
    "\n",
    "# find the right logfile for each profile\n",
    "for profile in table_names:\n",
    "    timestamp = profile[-13:]\n",
    "    # Convert timestamp to datetime object\n",
    "    date_time_obj = datetime.datetime.strptime(timestamp, '%Y%m%d-%H%M')\n",
    "    min_diff = datetime.timedelta(days=365*1000)  # initialize with a big time difference\n",
    "    closest_file = None\n",
    "\n",
    "    # Iterate over all files in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        # Check if filename is a Templog\n",
    "        if '__Templog.txt' in filename:\n",
    "            # Extract timestamp from filename and convert to datetime object\n",
    "            file_timestamp = filename[:16]\n",
    "            file_datetime = datetime.datetime.strptime(file_timestamp, '%Y%m%d_%Hh_%Mm')\n",
    "\n",
    "            # Calculate time difference\n",
    "            diff = abs(date_time_obj - file_datetime)\n",
    "\n",
    "            # If this file is closer, update min_diff and closest_file\n",
    "            if diff < min_diff:\n",
    "                min_diff = diff\n",
    "                closest_file = filename\n",
    "\n",
    "    if closest_file is None:\n",
    "        print(\"Logfile not found\")\n",
    "    else:\n",
    "        file_path = os.path.join(directory, closest_file)\n",
    "        file_size = os.path.getsize(file_path)  # Get file size in bytes\n",
    "        print(f\"Closest logfile: {closest_file}, Size: {file_size} bytes\")\n",
    "    \n",
    "    # Read the log file and parse the relevant data\n",
    "\n",
    "    df_log = create_log_df(file_path)\n",
    "\n",
    "    # Match the data with the profile dataframe\n",
    "    query = f'SELECT * FROM \"{profile}\"'\n",
    "    df = pd.read_sql_query(query, engine)\n",
    "    df.drop(['TT_x', 'T1_x', 'T2_x', 'TH_x', 'restart_x', 'relock_x', 'Time_log_x', 'TT_y', 'T1_y', 'T2_y', 'TH_y', 'restart_y', 'relock_y', 'Time_log_y', 'TT', 'T1', 'T2', 'TH', 'restart', 'relock', 'Time_log', 'TAG_event', 'TAG_event_x', 'TAG_event_y', 'near_event', 'near_event_x', 'near_event_y'], axis=1, inplace=True, errors='ignore')\n",
    "    # Convert the timestamps in both dataframes to datetime format\n",
    "    df['timestamp'] = df['date-time']\n",
    "    #df_log['timestamp_l'] = df_log['Time_log']\n",
    "\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], format='%Y%m%d-%H%M%S%f')\n",
    "    #df_log['timestamp_l'] = pd.to_datetime(df_log['timestamp_l'], format='%Y%m%d-%H%M%S')\n",
    "\n",
    "    # Sort the dataframes by the timestamp\n",
    "    df = df.sort_values('timestamp')\n",
    "    df_log = df_log.sort_values('timestamp')\n",
    "\n",
    "    # Use merge_asof to merge the two dataframes, finding the nearest match on the timestamp\n",
    "    df_combined = pd.merge_asof(df, df_log, left_on='timestamp', right_on='timestamp', direction='backward')\n",
    "    df_combined.drop('timestamp', axis=1, inplace=True)\n",
    "    #df_combined.drop('timestamp', axis=1, inplace=True)\n",
    "\n",
    "    #print(df_combined.head())\n",
    "\n",
    "    # Update database\n",
    "    df_combined.to_sql(profile, engine, if_exists='replace', index=False)\n",
    "    print(\"Updated {}\".format(profile))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15                         TT     T1     T2     TH  relock  restart\n",
      "timestamp                                                       \n",
      "2022-05-18 21:07:34  27.97  27.75  28.19  27.81    True    False\n",
      "2022-05-18 21:16:25  26.94  26.38  27.50  26.88    True    False\n",
      "2022-05-18 21:18:47  25.91  25.19  26.63  25.69    True    False\n",
      "2022-05-18 21:20:55  24.88  24.00  25.75  24.50    True    False\n",
      "2022-05-18 21:23:03  23.84  22.88  24.81  23.38    True    False\n",
      "2022-05-18 21:25:12  22.78  21.75  23.81  22.19    True    False\n",
      "2022-05-18 21:27:21  21.75  20.69  22.81  21.06    True    False\n",
      "2022-05-18 21:29:33  20.72  19.63  21.81  20.00    True    False\n",
      "2022-05-18 21:46:14  20.16  17.81  22.50  19.44    True    False\n",
      "2022-05-18 22:02:56  20.06  17.44  22.69  19.19    True    False\n",
      "2022-05-18 22:19:38  20.00  17.19  22.81  19.13    True    False\n",
      "2022-05-18 22:36:19  20.09  17.19  23.00  20.31    True    False\n",
      "2022-05-18 22:53:01  20.19  17.38  23.00  19.69    True    False\n",
      "2022-05-18 23:09:43  20.13  18.00  22.25  19.31    True    False\n",
      "2022-05-18 23:26:24  21.06  20.75  21.38  21.13    True    False\n"
     ]
    }
   ],
   "source": [
    "restart_rows = df[df['relock'] == True]\n",
    "print(len(restart_rows), restart_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Object id (per Profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for profile in tqdm(table_names):\n",
    "    query = f'SELECT * FROM \"{profile}\"'\n",
    "    df = pd.read_sql_query(query, engine)\n",
    "    \n",
    "    df.drop(['obj_id'], axis=1, inplace=True, errors='ignore')\n",
    "    sorted_df = df.sort_values(by='filename')\n",
    "    print(sorted_df.head())\n",
    "    sorted_fn_list = sorted_df['filename'].tolist()\n",
    "    obj_ids = []\n",
    "    id_cnt = 0\n",
    "    for img in sorted_fn_list:\n",
    "        curr_id = id_cnt\n",
    "        obj_ids.append('obj_'+str(curr_id))\n",
    "        id_cnt = id_cnt+1\n",
    "    sorted_df['obj_id'] = obj_ids\n",
    "    sorted_df.to_sql(profile, engine, if_exists='replace', index=False)\n",
    "    print(\"Updated {}\".format(profile))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate UMAP Embeddings for selected \"handcrafted\" features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UMAP(angular_rp_forest=True, metric='cosine', verbose=True)\n",
      "Wed Jun 12 14:36:01 2024 Construct fuzzy simplicial set\n",
      "Wed Jun 12 14:36:09 2024 Finding Nearest Neighbors\n",
      "Wed Jun 12 14:36:09 2024 Building RP forest with 59 trees\n",
      "Wed Jun 12 14:39:30 2024 NN descent for 20 iterations\n",
      "\t 1  /  20\n",
      "\t 2  /  20\n",
      "\tStopping threshold met -- exiting after 2 iterations\n",
      "Wed Jun 12 14:40:53 2024 Finished Nearest Neighbor Search\n",
      "Wed Jun 12 14:41:37 2024 Construct embedding\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eef1f11e016e44f8b7f854744d5205b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs completed:   0%|            0/200 [00:00]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jun 12 15:05:59 2024 Finished embedding\n"
     ]
    }
   ],
   "source": [
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "root_dir = '/media/plankton/30781fe1-cea5-4503-ae00-1986beb935d2/Segmentation_results/M181/results_240328'\n",
    "\n",
    "selected_features = ['pressure [dbar]', 'temperature', 'area', 'w', 'h', \n",
    "       'esd', 'interpolated_s', 'interpolated_o',\n",
    "       'interpolated_t', 'interpolated_chl', 'area_exc', 'area_rprops', '%area',\n",
    "       'major', 'minor', 'centroid_y', 'centroid_x', 'convex_area',\n",
    "       'min_intensity', 'max_intensity', 'mean_intensity', 'intden', 'perim.',\n",
    "       'elongation', 'range', 'perimareaexc', 'perimmajor', 'circex', 'angle',\n",
    "       'bounding_box_area', 'eccentricity', 'equivalent_diameter',\n",
    "       'euler_number', 'extent', 'local_centroid_col', 'local_centroid_row',\n",
    "       'solidity', 'TAG_event', 'part_based_filter']\n",
    "\n",
    "all_dfs = []  # list to store all the dataframes\n",
    "\n",
    "for profile in table_names:\n",
    "       query = f'SELECT * FROM \"{profile}\"'\n",
    "       df = pd.read_sql_query(query, engine)\n",
    "       df_selected = df[selected_features]\n",
    "       all_dfs.append(df_selected)  # append the selected dataframe to the list\n",
    "\n",
    "# combine all dataframes\n",
    "df_selected_comb = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "# #fit the UMAP model to the first profile\n",
    "# query = f'SELECT * FROM \"{table_names[0]}\"'\n",
    "# df = pd.read_sql_query(query, engine)\n",
    "# df_selected = df[selected_features]\n",
    "#print(df_selected.head())\n",
    "# Same for a StandardScaler instance\n",
    "scaler = StandardScaler().fit(df_selected_comb)\n",
    "df_selected_scaled = scaler.transform(df_selected_comb)\n",
    "#print(df_selected_scaled)\n",
    "reducer = umap.UMAP(metric='cosine', verbose=True).fit(df_selected_scaled)\n",
    "\n",
    "for profile in table_names:\n",
    "       query = f'SELECT * FROM \"{profile}\"'\n",
    "       df = pd.read_sql_query(query, engine)\n",
    "       df_selected = df[selected_features]\n",
    "       \n",
    "       # transform the scaler to the selected features\n",
    "       df_selected_scaled = scaler.transform(df_selected)\n",
    "\n",
    "       # Then transform the UMAP model with the scaled data\n",
    "       embedding = reducer.transform(df_selected_scaled) \n",
    "\n",
    "       #plot embedding \n",
    "       plt.scatter(embedding[:, 0], embedding[:, 1])\n",
    "       plt.gca().set_aspect('equal', 'datalim')\n",
    "       plt.title('UMAP projection of the selected features', fontsize=18)\n",
    "       plt.show()\n",
    "       \n",
    "       #add embedding to database\n",
    "       df.drop(['umap_x', 'umap_y'], axis=1, inplace=True, errors='ignore')\n",
    "       df['umap_x']=embedding[:, 0]\n",
    "       df['umap_y']=embedding[:, 1]\n",
    "       df.to_sql(profile, engine, if_exists='replace', index=False)\n",
    "       print(\"Updated {}\".format(profile))\n",
    "\n",
    "       #save embedding to pickle file\n",
    "       # profile = profile.replace('deg', '°')  \n",
    "       # dest = os.path.join(root_dir, profile, 'handcrafted_embedding.pkl')\n",
    "       # with open(dest, 'wb') as f:\n",
    "       #        pickle.dump(embedding, f)\n",
    "       # print(f'UMAP embedding saved to {dest}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add embedding to df afterwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated M181-245-1_CTD-065_00deg00S-031deg00W_20220514-0907\n",
      "Updated M181-200-1_CTD-055_00deg00S-023deg00W_20220511-0023\n",
      "Updated M181-227-1_CTD-060_00deg00S-027deg00W_20220512-1748\n",
      "Updated M181-126-1_CTD-040_00deg00S-010deg00W_20220505-1401\n",
      "Updated M181-107-1_CTD-036_00deg00S-007deg00W_20220504-0756\n",
      "Updated M181-175-1_CTD-050_00deg00S-019deg00W_20220509-0543\n",
      "Updated M181-267-1_CTD-070_00deg00S-035deg00W_20220516-0335\n",
      "Updated M181-140-1_CTD-043_00deg00S-013deg00W_20220506-1822\n",
      "Updated M181-160-1_CTD-047_00deg00S-016deg00W_20220508-0038\n",
      "Updated M181-285-1_CTD-075_00deg00S-039deg00W_20220517-2024\n",
      "Updated M181-295-1_CTD-080_00deg00S-041deg00W_20220518-2053\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "for profile in table_names:\n",
    "       query = f'SELECT * FROM \"{profile}\"'\n",
    "       df = pd.read_sql_query(query, engine)\n",
    "       features_loc = os.path.dirname(os.path.dirname(df['overview_path'].iloc[0])) + '/handcrafted_embedding.pkl'\n",
    "       with open(features_loc, 'rb') as f:\n",
    "              embedding = pickle.load(f)\n",
    "       df['umap_x']=embedding[:, 0]\n",
    "       df['umap_y']=embedding[:, 1]\n",
    "       df.to_sql(profile, engine, if_exists='replace', index=False)\n",
    "       print(\"Updated {}\".format(profile))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add prediction of schlieren images by filtering based on particle counts per image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated M181-160-1_CTD-047_00deg00S-016deg00W_20220508-0038\n",
      "Updated M181-245-1_CTD-065_00deg00S-031deg00W_20220514-0907\n",
      "Updated M181-200-1_CTD-055_00deg00S-023deg00W_20220511-0023\n",
      "Updated M181-227-1_CTD-060_00deg00S-027deg00W_20220512-1748\n",
      "Updated M181-126-1_CTD-040_00deg00S-010deg00W_20220505-1401\n",
      "Updated M181-107-1_CTD-036_00deg00S-007deg00W_20220504-0756\n",
      "Updated M181-175-1_CTD-050_00deg00S-019deg00W_20220509-0543\n",
      "Updated M181-267-1_CTD-070_00deg00S-035deg00W_20220516-0335\n",
      "Updated M181-140-1_CTD-043_00deg00S-013deg00W_20220506-1822\n",
      "Updated M181-285-1_CTD-075_00deg00S-039deg00W_20220517-2024\n",
      "Updated M181-295-1_CTD-080_00deg00S-041deg00W_20220518-2053\n"
     ]
    }
   ],
   "source": [
    "for profile in table_names:\n",
    "    query = f'SELECT * FROM \"{profile}\"'\n",
    "    df = pd.read_sql_query(query, engine)\n",
    "    df.drop(['part_based_filter'], axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "    df_unique = df[['date-time', 'pressure [dbar]', 'depth [m]', 'img_id','temperature','overview_path','interpolated_s','interpolated_t','interpolated_o','interpolated_chl','interpolated_z_factor','restart','relock','TAG_event']].drop_duplicates()\n",
    "    df_count = df.groupby('date-time').size().reset_index(name='count')\n",
    "    df_unique = df_unique.merge(df_count, on='date-time', how='left')\n",
    "    df_unique = df_unique.sort_values('pressure [dbar]')\n",
    "\n",
    "\n",
    "    # Filter the data\n",
    "    df_unique['part_based_filter'] = df_unique['count'].apply(lambda x: 0 if x < df_unique['count'].std() else 1)\n",
    "    #print(df_unique.head())\n",
    "    filtered_df = df_unique[df_unique['part_based_filter'] == 0]\n",
    "\n",
    "    # Merge 'df_unique' back to 'df' to create 'part_based_filter' column in 'df'\n",
    "    df = df.merge(df_unique[['date-time', 'part_based_filter']], on='date-time', how='left')\n",
    "    df.to_sql(profile, engine, if_exists='replace', index=False)\n",
    "    print(df['part_based_filter'].value_counts())\n",
    "    print(\"Updated {}\".format(profile))\n",
    "    #print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           date-time  pressure [dbar]  depth [m]  temperature  index  img_id  \\\n",
      "0  20220506-18475929            -0.08      -0.08        32.27   1005       1   \n",
      "1  20220506-18475929            -0.08      -0.08        32.27   1012       1   \n",
      "2  20220506-18475929            -0.08      -0.08        32.27    103       1   \n",
      "3  20220506-18475929            -0.08      -0.08        32.27   1040       1   \n",
      "4  20220506-18475929            -0.08      -0.08        32.27    105       1   \n",
      "\n",
      "                                       filename    area       x       y  ...  \\\n",
      "0  20220506-18475929_000.992bar_32.27C_1005.png  9945.0  2102.5   266.5  ...   \n",
      "1  20220506-18475929_000.992bar_32.27C_1012.png   459.5   508.5   202.5  ...   \n",
      "2   20220506-18475929_000.992bar_32.27C_103.png   508.5  2476.5  2105.5  ...   \n",
      "3  20220506-18475929_000.992bar_32.27C_1040.png   690.0   362.0   136.0  ...   \n",
      "4   20220506-18475929_000.992bar_32.27C_105.png   440.0   445.5  2097.5  ...   \n",
      "\n",
      "      TT     T1     T2     TH  relock restart  obj_id     umap_x     umap_y  \\\n",
      "0  32.72  32.38  33.06  32.44   False   False   obj_0   1.634831  -5.788402   \n",
      "1  32.72  32.38  33.06  32.44   False   False   obj_1  12.564866  -5.910573   \n",
      "2  32.72  32.38  33.06  32.44   False   False   obj_2  14.058960  -0.872380   \n",
      "3  32.72  32.38  33.06  32.44   False   False   obj_3  15.630560  10.468481   \n",
      "4  32.72  32.38  33.06  32.44   False   False   obj_4   8.701097  -4.627170   \n",
      "\n",
      "   part_based_filter  \n",
      "0                  1  \n",
      "1                  1  \n",
      "2                  1  \n",
      "3                  1  \n",
      "4                  1  \n",
      "\n",
      "[5 rows x 64 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df['part_based_filter'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export images to a directory and zip it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded images from M181-175-1_CTD-050_00deg00S-019deg00W_20220509-0543\n",
      "Loaded images from M181-267-1_CTD-070_00deg00S-035deg00W_20220516-0335\n",
      "Loaded images from M181-140-1_CTD-043_00deg00S-013deg00W_20220506-1822\n",
      "Loaded images from M181-160-1_CTD-047_00deg00S-016deg00W_20220508-0038\n",
      "Loaded images from M181-245-1_CTD-065_00deg00S-031deg00W_20220514-0907\n",
      "Loaded images from M181-200-1_CTD-055_00deg00S-023deg00W_20220511-0023\n",
      "Loaded images from M181-227-1_CTD-060_00deg00S-027deg00W_20220512-1748\n",
      "Loaded images from M181-126-1_CTD-040_00deg00S-010deg00W_20220505-1401\n",
      "Loaded images from M181-285-1_CTD-075_00deg00S-039deg00W_20220517-2024\n",
      "Loaded images from M181-295-1_CTD-080_00deg00S-041deg00W_20220518-2053\n",
      "Loaded images from M181-107-1_CTD-036_00deg00S-007deg00W_20220504-0756\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from PIL import Image\n",
    "\n",
    "# create a new directory for the images\n",
    "os.makedirs('/media/plankton/30781fe1-cea5-4503-ae00-1986beb935d2/exported_images', exist_ok=True)\n",
    "df_all = pd.DataFrame()\n",
    "\n",
    "for profile in table_names:\n",
    "    query = f'SELECT * FROM \"{profile}\"'\n",
    "    df = pd.read_sql_query(query, engine)\n",
    "    # go through the DataFrame\n",
    "    df_filtered = df[(df['TAG_event'] == 0) & (df['part_based_filter'] == 0)]\n",
    "    # for idx, row in df_filtered.iterrows():\n",
    "    #     # read the image\n",
    "    #     img = Image.open(row['full_path'])\n",
    "    #     # save to the new location\n",
    "    #     img.save(os.path.join('/media/plankton/30781fe1-cea5-4503-ae00-1986beb935d2/exported_images', os.path.basename(row['full_path'])))\n",
    "    print(\"Loaded images from {}\".format(profile))\n",
    "    df_all = pd.concat([df_all, df_filtered])\n",
    "\n",
    "# save df_all to a csv file\n",
    "df_all.to_csv('/media/plankton/30781fe1-cea5-4503-ae00-1986beb935d2/exported_images.csv')\n",
    "\n",
    "# make a zip file\n",
    "#shutil.make_archive('/media/plankton/30781fe1-cea5-4503-ae00-1986beb935d2/exported_images', 'zip', '/media/plankton/30781fe1-cea5-4503-ae00-1986beb935d2/exported_images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract crops from raw images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded images from M181-175-1_CTD-050_00deg00S-019deg00W_20220509-0543\n",
      "Loaded images from M181-267-1_CTD-070_00deg00S-035deg00W_20220516-0335\n",
      "Loaded images from M181-140-1_CTD-043_00deg00S-013deg00W_20220506-1822\n",
      "Loaded images from M181-160-1_CTD-047_00deg00S-016deg00W_20220508-0038\n",
      "Loaded images from M181-245-1_CTD-065_00deg00S-031deg00W_20220514-0907\n",
      "Loaded images from M181-200-1_CTD-055_00deg00S-023deg00W_20220511-0023\n",
      "Loaded images from M181-227-1_CTD-060_00deg00S-027deg00W_20220512-1748\n",
      "Loaded images from M181-126-1_CTD-040_00deg00S-010deg00W_20220505-1401\n",
      "Loaded images from M181-285-1_CTD-075_00deg00S-039deg00W_20220517-2024\n",
      "Loaded images from M181-295-1_CTD-080_00deg00S-041deg00W_20220518-2053\n",
      "Loaded images from M181-107-1_CTD-036_00deg00S-007deg00W_20220504-0756\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from PIL import Image\n",
    "\n",
    "raw_root = '/mnt/m181'\n",
    "\n",
    "# scale the coordinates and the size of the crop\n",
    "scale_factor = 5120 / 2560  # target_resolution / original_resolution\n",
    "\n",
    "for profile in table_names:\n",
    "    image_name_old = ''\n",
    "    query = f'SELECT * FROM \"{profile}\"'\n",
    "    df = pd.read_sql_query(query, engine)\n",
    "    # go through the DataFrame\n",
    "    df_filtered = df[(df['TAG_event'] == 0) & (df['part_based_filter'] == 0)]\n",
    "    for idx, row in df_filtered.iterrows():\n",
    "        # read the image\n",
    "        img_name = os.path.basename(row['overview_path'])\n",
    "        if img_name != image_name_old:            \n",
    "            raw_img_path = os.path.join(raw_root, profile, 'PNG', img_name)\n",
    "            img = Image.open(raw_img_path)\n",
    "            image_name_old = img_name\n",
    "        \n",
    "        x = row['x']\n",
    "        y = row['y']\n",
    "        w = row['w']\n",
    "        h = row['h']\n",
    "\n",
    "        \n",
    "        x = x * scale_factor\n",
    "        y = y * scale_factor\n",
    "        w = w * scale_factor\n",
    "        h = h * scale_factor\n",
    "\n",
    "        # calculate the top left corner of the crop\n",
    "        x1 = x - w / 2\n",
    "        y1 = y - h / 2\n",
    "\n",
    "        # calculate the bottom right corner of the crop\n",
    "        x2 = x + w / 2\n",
    "        y2 = y + h / 2\n",
    "\n",
    "        # crop the image\n",
    "        cropped_image = img.crop((x1, y1, x2, y2))\n",
    "\n",
    "        # save the cropped image\n",
    "        cropped_image.save(os.path.join('/media/plankton/30781fe1-cea5-4503-ae00-1986beb935d2/exported_images_raw', os.path.basename(row['full_path'])))\n",
    "    print(\"Loaded images from {}\".format(profile))\n",
    "shutil.make_archive('/media/plankton/30781fe1-cea5-4503-ae00-1986beb935d2/exported_images_raw', 'zip', '/media/plankton/30781fe1-cea5-4503-ae00-1986beb935d2/exported_images_raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export binary masks to zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded images from M181-175-1_CTD-050_00deg00S-019deg00W_20220509-0543\n",
      "Loaded images from M181-267-1_CTD-070_00deg00S-035deg00W_20220516-0335\n",
      "Loaded images from M181-140-1_CTD-043_00deg00S-013deg00W_20220506-1822\n",
      "Loaded images from M181-160-1_CTD-047_00deg00S-016deg00W_20220508-0038\n",
      "Loaded images from M181-245-1_CTD-065_00deg00S-031deg00W_20220514-0907\n",
      "Loaded images from M181-200-1_CTD-055_00deg00S-023deg00W_20220511-0023\n",
      "Loaded images from M181-227-1_CTD-060_00deg00S-027deg00W_20220512-1748\n",
      "Loaded images from M181-126-1_CTD-040_00deg00S-010deg00W_20220505-1401\n",
      "Loaded images from M181-285-1_CTD-075_00deg00S-039deg00W_20220517-2024\n",
      "Loaded images from M181-295-1_CTD-080_00deg00S-041deg00W_20220518-2053\n",
      "Loaded images from M181-107-1_CTD-036_00deg00S-007deg00W_20220504-0756\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/media/plankton/30781fe1-cea5-4503-ae00-1986beb935d2/exported_images_masks.zip'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from PIL import Image\n",
    "\n",
    "# create a new directory for the images\n",
    "os.makedirs('/media/plankton/30781fe1-cea5-4503-ae00-1986beb935d2/exported_images_masks', exist_ok=True)\n",
    "df_all = pd.DataFrame()\n",
    "\n",
    "for profile in table_names:\n",
    "    query = f'SELECT * FROM \"{profile}\"'\n",
    "    df = pd.read_sql_query(query, engine)\n",
    "    # go through the DataFrame\n",
    "    df_filtered = df[(df['TAG_event'] == 0) & (df['part_based_filter'] == 0)]\n",
    "    for idx, row in df_filtered.iterrows():\n",
    "        mask_name = os.path.basename(row['full_path'])\n",
    "        mask_name = mask_name[:-4] + '_mask.png'\n",
    "        mask_path = os.path.join(os.path.dirname(os.path.dirname(row['full_path'])), 'Masks', mask_name)\n",
    "        # read the image\n",
    "        img = Image.open(mask_path)\n",
    "        # save to the new location\n",
    "        img.save(os.path.join('/media/plankton/30781fe1-cea5-4503-ae00-1986beb935d2/exported_images_masks', mask_name))\n",
    "    print(\"Loaded images from {}\".format(profile))\n",
    "    df_all = pd.concat([df_all, df_filtered])\n",
    "\n",
    "# save df_all to a csv file\n",
    "df_all.to_csv('/media/plankton/30781fe1-cea5-4503-ae00-1986beb935d2/exported_images.csv')\n",
    "\n",
    "# make a zip file\n",
    "shutil.make_archive('/media/plankton/30781fe1-cea5-4503-ae00-1986beb935d2/exported_images_masks', 'zip', '/media/plankton/30781fe1-cea5-4503-ae00-1986beb935d2/exported_images_masks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.dispose()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
